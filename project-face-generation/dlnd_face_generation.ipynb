{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Generation\n",
    "\n",
    "In this project, you'll define and train a DCGAN on a dataset of faces. Your goal is to get a generator network to generate *new* images of faces that look as realistic as possible!\n",
    "\n",
    "The project will be broken down into a series of tasks from **loading in data to defining and training adversarial networks**. At the end of the notebook, you'll be able to visualize the results of your trained Generator to see how it performs; your generated samples should look like fairly realistic faces with small amounts of noise.\n",
    "\n",
    "### Get the Data\n",
    "\n",
    "You'll be using the [CelebFaces Attributes Dataset (CelebA)](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) to train your adversarial networks.\n",
    "\n",
    "This dataset is more complex than the number datasets (like MNIST or SVHN) you've been working with, and so, you should prepare to define deeper networks and train them for a longer time to get good results. It is suggested that you utilize a GPU for training.\n",
    "\n",
    "### Pre-processed Data\n",
    "\n",
    "Since the project's main focus is on building the GANs, we've done *some* of the pre-processing for you. Each of the CelebA images has been cropped to remove parts of the image that don't include a face, then resized down to 64x64x3 NumPy images. Some sample data is show below.\n",
    "\n",
    "<img src='assets/processed_face_data.png' width=60% />\n",
    "\n",
    "> If you are working locally, you can download this data [by clicking here](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/November/5be7eb6f_processed-celeba-small/processed-celeba-small.zip)\n",
    "\n",
    "This is a zip file that you'll need to extract in the home directory of this notebook for further loading and processing. After extracting the data, you should be left with a directory of data `processed_celeba_small/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can comment out after executing\n",
    "!unzip processed_celeba_small.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'processed_celeba_small/'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "#import helper\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the CelebA Data\n",
    "\n",
    "The [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset contains over 200,000 celebrity images with annotations. Since you're going to be generating faces, you won't need the annotations, you'll only need the images. Note that these are color images with [3 color channels (RGB)](https://en.wikipedia.org/wiki/Channel_(digital_image)#RGB_Images) each.\n",
    "\n",
    "### Pre-process and Load the Data\n",
    "\n",
    "Since the project's main focus is on building the GANs, we've done *some* of the pre-processing for you. Each of the CelebA images has been cropped to remove parts of the image that don't include a face, then resized down to 64x64x3 NumPy images. This *pre-processed* dataset is a smaller subset of the very large CelebA data.\n",
    "\n",
    "> There are a few other steps that you'll need to **transform** this data and create a **DataLoader**.\n",
    "\n",
    "#### Exercise: Complete the following `get_dataloader` function, such that it satisfies these requirements:\n",
    "\n",
    "* Your images should be square, Tensor images of size `image_size x image_size` in the x and y dimension.\n",
    "* Your function should return a DataLoader that shuffles and batches these Tensor images.\n",
    "\n",
    "#### ImageFolder\n",
    "\n",
    "To create a dataset given a directory of images, it's recommended that you use PyTorch's [ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) wrapper, with a root directory `processed_celeba_small/` and data transformation passed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(batch_size, image_size, data_dir='processed_celeba_small/'):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    :param batch_size: The size of each batch; the number of images in a batch\n",
    "    :param img_size: The square size of the image data (x, y)\n",
    "    :param data_dir: Directory where image data is located\n",
    "    :return: DataLoader with batched data\n",
    "    \"\"\"\n",
    "        \n",
    "    # Resize the image --> should be image_size x image_size\n",
    "    \n",
    "    data_transforms = transforms.Compose([transforms.Resize(image_size), \n",
    "                                          transforms.ToTensor()\n",
    "                                         ])\n",
    "    \n",
    "    face_train      = datasets.ImageFolder(root      = data_dir, \n",
    "                                           transform = data_transforms\n",
    "                                          )\n",
    "    \n",
    "    train_loader    = torch.utils.data.DataLoader(dataset    = face_train,\n",
    "                                                  batch_size = batch_size,\n",
    "                                                  shuffle    = True\n",
    "                                                 )\n",
    "    \n",
    "    return train_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a DataLoader\n",
    "\n",
    "#### Exercise: Create a DataLoader `celeba_train_loader` with appropriate hyperparameters.\n",
    "\n",
    "Call the above function and create a dataloader to view images. \n",
    "* You can decide on any reasonable `batch_size` parameter\n",
    "* Your `image_size` **must be** `32`. Resizing the data to a smaller size will make for faster training, while still creating convincing images of faces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function hyperparameters\n",
    "batch_size = 64\n",
    "img_size   = 32\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# Call your function and get a dataloader\n",
    "celeba_train_loader = get_dataloader(batch_size, img_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can view some images! You should seen square images of somewhat-centered faces.\n",
    "\n",
    "Note: You'll need to convert the Tensor images into a NumPy type and transpose the dimensions to correctly display an image, suggested `imshow` code is below, but it may not be perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper display function\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# obtain one batch of training images\n",
    "dataiter = iter(celeba_train_loader)\n",
    "images, _ = dataiter.next() # _ for no labels\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "plot_size=20\n",
    "for idx in np.arange(plot_size):\n",
    "    ax = fig.add_subplot(2, plot_size/2, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Pre-process your image data and scale it to a pixel range of -1 to 1\n",
    "\n",
    "You need to do a bit of pre-processing; you know that the output of a `tanh` activated generator will contain pixel values in a range from -1 to 1, and so, we need to rescale our training images to a range of -1 to 1. (Right now, they are in a range from 0-1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the scale function\n",
    "def scale(x, feature_range=(-1, 1)):\n",
    "    ''' Scale takes in an image x and returns that image, scaled\n",
    "       with a feature_range of pixel values from -1 to 1. \n",
    "       This function assumes that the input x is already scaled from 0-1.'''\n",
    "    \n",
    "    min, max = feature_range\n",
    "    x = x * (max - min) + min\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "# check scaled range\n",
    "# should be close to -1 to 1\n",
    "img = images[0]\n",
    "scaled_img = scale(img)\n",
    "\n",
    "print('Min: ', scaled_img.min())\n",
    "print('Max: ', scaled_img.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Define the Model\n",
    "\n",
    "A GAN is comprised of two adversarial networks, a discriminator and a generator.\n",
    "\n",
    "## Discriminator\n",
    "\n",
    "Your first task will be to define the discriminator. This is a convolutional classifier like you've built before, only without any maxpooling layers. To deal with this complex data, it's suggested you use a deep network with **normalization**. You are also allowed to create any helper functions that may be useful.\n",
    "\n",
    "#### Exercise: Complete the Discriminator class\n",
    "* The inputs to the discriminator are 32x32x3 tensor images\n",
    "* The output should be a single value that will indicate whether a given image is real or fake\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper conv function\n",
    "def conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n",
    "    \"\"\"Creates a convolutional layer, with optional batch normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    layers     = []\n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, \n",
    "                           kernel_size, stride, padding, bias=False)\n",
    "    \n",
    "    # append conv layer\n",
    "    layers.append(conv_layer)\n",
    "\n",
    "    if batch_norm:\n",
    "        # append batchnorm layer\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "    seq_layers = nn.Sequential(*layers)\n",
    "     \n",
    "    # using Sequential container\n",
    "    return seq_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_dim):\n",
    "        \"\"\"\n",
    "        Initialize the Discriminator Module\n",
    "        :param conv_dim: The depth of the first convolutional layer\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.conv_dim = conv_dim\n",
    "        \n",
    "        # 3 conv layers\n",
    "        self.conv1 = conv(3,conv_dim, 4, batch_norm=False)\n",
    "        self.conv2 = conv(conv_dim,conv_dim*2,4)\n",
    "        self.conv3 = conv(conv_dim*2,conv_dim*4,4)\n",
    "        \n",
    "        # fc\n",
    "        self.fc = nn.Linear(conv_dim*4*4*4,1)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param x: The input to the neural network     \n",
    "        :return: Discriminator logits; the output of the neural network\n",
    "        \"\"\"\n",
    "        \n",
    "        # HL + activation\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
    "        x = F.leaky_relu(self.conv2(x), 0.2)\n",
    "        x = F.leaky_relu(self.conv3(x), 0.2)\n",
    "        \n",
    "        x = x.view(-1, self.conv_dim*4*4*4)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_discriminator(Discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "The generator should upsample an input and generate a *new* image of the same size as our training data `32x32x3`. This should be mostly transpose convolutional layers with normalization applied to the outputs.\n",
    "\n",
    "#### Exercise: Complete the Generator class\n",
    "* The inputs to the generator are vectors of some length `z_size`\n",
    "* The output should be a image of shape `32x32x3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper deconv function\n",
    "def deconv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n",
    "    \"\"\"Creates a transposed-convolutional layer, with optional batch normalization.\n",
    "    \"\"\"\n",
    "    \n",
    "    layers = []\n",
    "    tcl    = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "    \n",
    "    layers.append(tcl)\n",
    "    \n",
    "    if batch_norm:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "    seq_layers = nn.Sequential(*layers)\n",
    "    \n",
    "    return seq_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_size, conv_dim):\n",
    "        \"\"\"\n",
    "        Initialize the Generator Module\n",
    "        :param z_size: The length of the input latent vector, z\n",
    "        :param conv_dim: The depth of the inputs to the *last* transpose convolutional layer\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.conv_dim = conv_dim\n",
    "        \n",
    "        # 3 tcl\n",
    "        self.t_conv1 = deconv(conv_dim*4,conv_dim*2,4)\n",
    "        self.t_conv2 = deconv(conv_dim*2, conv_dim, 4)\n",
    "        self.t_conv3 = deconv(conv_dim, 3, 4, batch_norm=False)\n",
    "        \n",
    "        # final fc\n",
    "        self.fc = nn.Linear(z_size, conv_dim*4*4*4)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param x: The input to the neural network     \n",
    "        :return: A 32x32x3 Tensor image as output\n",
    "        \"\"\"\n",
    "        # define feedforward behavior\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1,self.conv_dim*4, 4, 4)\n",
    "\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.relu(self.t_conv2(x))\n",
    "        x = F.relu(self.t_conv3(x))\n",
    "        \n",
    "        x = F.tanh(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_generator(Generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the weights of your networks\n",
    "\n",
    "To help your models converge, you should initialize the weights of the convolutional and linear layers in your model. From reading the [original DCGAN paper](https://arxiv.org/pdf/1511.06434.pdf), they say:\n",
    "> All weights were initialized from a zero-centered Normal distribution with standard deviation 0.02.\n",
    "\n",
    "So, your next task will be to define a weight initialization function that does just this!\n",
    "\n",
    "You can refer back to the lesson on weight initialization or even consult existing model code, such as that from [the `networks.py` file in CycleGAN Github repository](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py) to help you complete this function.\n",
    "\n",
    "#### Exercise: Complete the weight initialization function\n",
    "\n",
    "* This should initialize only **convolutional** and **linear** layers\n",
    "* Initialize the weights to a normal distribution, centered around 0, with a standard deviation of 0.02.\n",
    "* The bias terms, if they exist, may be left alone or set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    \"\"\"\n",
    "    Applies initial weights to certain layers in a model .\n",
    "    The weights are taken from a normal distribution \n",
    "    with mean = 0, std dev = 0.02.\n",
    "    :param m: A module or layer in a network    \n",
    "    \"\"\"\n",
    "    # classname will be something like:\n",
    "    # `Conv`, `BatchNorm2d`, `Linear`, etc.\n",
    "    classname = m.__class__.__name__\n",
    "    \n",
    "    if (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "        init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build complete network\n",
    "\n",
    "Define your models' hyperparameters and instantiate the discriminator and generator from the classes defined above. Make sure you've passed in the correct input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "def build_network(d_conv_dim, g_conv_dim, z_size):\n",
    "    # define discriminator and generator\n",
    "    D = Discriminator(d_conv_dim)\n",
    "    G = Generator(z_size=z_size, conv_dim=g_conv_dim)\n",
    "\n",
    "    # initialize model weights\n",
    "    D.apply(weights_init_normal)\n",
    "    G.apply(weights_init_normal)\n",
    "\n",
    "    print(D)\n",
    "    print()\n",
    "    print(G)\n",
    "    \n",
    "    return D, G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Define model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (fc): Linear(in_features=4096, out_features=1, bias=True)\n",
      ")\n",
      "\n",
      "Generator(\n",
      "  (t_conv1): Sequential(\n",
      "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (t_conv2): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (t_conv3): Sequential(\n",
      "    (0): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=100, out_features=4096, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model hyperparams\n",
    "d_conv_dim = 64\n",
    "g_conv_dim = 64\n",
    "z_size     = 100\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "D, G = build_network(d_conv_dim, g_conv_dim, z_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on GPU\n",
    "\n",
    "Check if you can train on GPU. Here, we'll set this as a boolean variable `train_on_gpu`. Later, you'll be responsible for making sure that \n",
    ">* Models,\n",
    "* Model inputs, and\n",
    "* Loss function arguments\n",
    "\n",
    "Are moved to GPU, where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Training on GPU!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Discriminator and Generator Losses\n",
    "\n",
    "Now we need to calculate the losses for both types of adversarial networks.\n",
    "\n",
    "### Discriminator Losses\n",
    "\n",
    "> * For the discriminator, the total loss is the sum of the losses for real and fake images, `d_loss = d_real_loss + d_fake_loss`. \n",
    "* Remember that we want the discriminator to output 1 for real images and 0 for fake images, so we need to set up the losses to reflect that.\n",
    "\n",
    "\n",
    "### Generator Loss\n",
    "\n",
    "The generator loss will look similar only with flipped labels. The generator's goal is to get the discriminator to *think* its generated images are *real*.\n",
    "\n",
    "#### Exercise: Complete real and fake loss functions\n",
    "\n",
    "**You may choose to use either cross entropy or a least squares error loss to complete the following `real_loss` and `fake_loss` functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_loss(D_out):\n",
    "    '''Calculates how close discriminator outputs are to being real.\n",
    "       param, D_out: discriminator logits\n",
    "       return: real loss'''\n",
    "    loss = torch.mean((D_out - 1)**2)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def fake_loss(D_out):\n",
    "    '''Calculates how close discriminator outputs are to being fake.\n",
    "       param, D_out: discriminator logits\n",
    "       return: fake loss'''\n",
    "    loss = torch.mean((D_out)**2)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "#### Exercise: Define optimizers for your Discriminator (D) and Generator (G)\n",
    "\n",
    "Define optimizers for your models with appropriate hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "beta1= 0.1\n",
    "beta2= 0.999\n",
    "\n",
    "# Create optimizers for the discriminator D and generator G\n",
    "d_optimizer = optim.Adam(D.parameters(), lr, [beta1, beta2])\n",
    "g_optimizer = optim.Adam(G.parameters(), lr, [beta1, beta2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training\n",
    "\n",
    "Training will involve alternating between training the discriminator and the generator. You'll use your functions `real_loss` and `fake_loss` to help you calculate the discriminator losses.\n",
    "\n",
    "* You should train the discriminator by alternating on real and fake images\n",
    "* Then the generator, which tries to trick the discriminator and should have an opposing loss function\n",
    "\n",
    "\n",
    "#### Saving Samples\n",
    "\n",
    "You've been given some code to print out some loss statistics and save some generated \"fake\" samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise: Complete the training function\n",
    "\n",
    "Keep in mind that, if you've moved your models to GPU, you'll also have to move any model inputs to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(D, G, n_epochs, print_every=50):\n",
    "    '''Trains adversarial networks for some number of epochs\n",
    "       param, D: the discriminator network\n",
    "       param, G: the generator network\n",
    "       param, n_epochs: number of epochs to train for\n",
    "       param, print_every: when to print and record the models' losses\n",
    "       return: D and G losses'''\n",
    "    \n",
    "    # move models to GPU\n",
    "    if train_on_gpu:\n",
    "        D.cuda()\n",
    "        G.cuda()\n",
    "\n",
    "    # keep track of loss and generated, \"fake\" samples\n",
    "    samples = []\n",
    "    losses = []\n",
    "\n",
    "    # Get some fixed data for sampling. These are images that are held\n",
    "    # constant throughout training, and allow us to inspect the model's performance\n",
    "    sample_size=16\n",
    "    fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
    "    fixed_z = torch.from_numpy(fixed_z).float()\n",
    "    # move z to GPU if available\n",
    "    if train_on_gpu:\n",
    "        fixed_z = fixed_z.cuda()\n",
    "\n",
    "    # epoch training loop\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # batch training loop\n",
    "        for batch_i, (real_images, _) in enumerate(celeba_train_loader):\n",
    "\n",
    "            batch_size = real_images.size(0)\n",
    "            real_images = scale(real_images)\n",
    "\n",
    "            # ===============================================\n",
    "            #         YOUR CODE HERE: TRAIN THE NETWORKS\n",
    "            # ===============================================\n",
    "            \n",
    "            # 1. Train the discriminator on real and fake images\n",
    "            \n",
    "            # ============================================\n",
    "            #            TRAIN THE DISCRIMINATOR\n",
    "            # ============================================\n",
    "            \n",
    "            ####################\n",
    "            # With real images #\n",
    "            ####################\n",
    "            d_optimizer.zero_grad()\n",
    "            \n",
    "            if train_on_gpu:\n",
    "                real_images = real_images.cuda()\n",
    "\n",
    "            d_real      = D(real_images)\n",
    "            d_real_loss = real_loss(d_real)\n",
    "            \n",
    "            ####################\n",
    "            # With fake images #\n",
    "            ####################\n",
    "            # Generate z vector\n",
    "            z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "            z = torch.from_numpy(z).float()\n",
    "        \n",
    "            # Generate image\n",
    "            if train_on_gpu:\n",
    "                z = z.cuda()\n",
    "                \n",
    "            fake_images = G(z)\n",
    "\n",
    "            d_fake      = D(fake_images)\n",
    "            d_fake_loss = fake_loss(d_fake)\n",
    "            \n",
    "            ##########\n",
    "            # D loss #\n",
    "            ##########\n",
    "            \n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            \n",
    "            ########################\n",
    "            # Backward + Optomizer #\n",
    "            ########################\n",
    "            \n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            # ============================================\n",
    "            #            TRAIN THE DISCRIMINATOR\n",
    "            # ============================================\n",
    "            \n",
    "            # 2. Train the generator with an adversarial loss\n",
    "            g_optimizer.zero_grad()\n",
    "            \n",
    "            #######################\n",
    "            # Generate fake image #\n",
    "            #######################\n",
    "            \n",
    "            # Generate z vector\n",
    "            z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "            z = torch.from_numpy(z).float()\n",
    "            \n",
    "            if train_on_gpu:\n",
    "                z = z.cuda()\n",
    "            \n",
    "            fake_images = G(z)\n",
    "                    \n",
    "            #############\n",
    "            # real_loss #\n",
    "            #############\n",
    "            g_fake = D(fake_images)\n",
    "            g_loss = real_loss(g_fake)\n",
    "            \n",
    "            ########################\n",
    "            # Backward + Optomizer #\n",
    "            ########################\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            # ===============================================\n",
    "            #              END OF YOUR CODE\n",
    "            # ===============================================\n",
    "\n",
    "            # Print some loss stats\n",
    "            if batch_i % print_every == 0:\n",
    "                # append discriminator loss and generator loss\n",
    "                losses.append((d_loss.item(), g_loss.item()))\n",
    "                # print discriminator and generator loss\n",
    "                print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
    "                        epoch+1, n_epochs, d_loss.item(), g_loss.item()))\n",
    "\n",
    "\n",
    "        ## AFTER EACH EPOCH##    \n",
    "        # this code assumes your generator is named G, feel free to change the name\n",
    "        # generate and save sample, fake images\n",
    "        G.eval() # for generating samples\n",
    "        samples_z = G(fixed_z)\n",
    "        samples.append(samples_z)\n",
    "        G.train() # back to training mode\n",
    "\n",
    "    # Save training generator samples\n",
    "    with open('train_samples.pkl', 'wb') as f:\n",
    "        pkl.dump(samples, f)\n",
    "    \n",
    "    # finally return losses\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your number of training epochs and train your GAN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [    1/   30] | d_loss: 0.9122 | g_loss: 2.2341\n",
      "Epoch [    1/   30] | d_loss: 0.0973 | g_loss: 0.8304\n",
      "Epoch [    1/   30] | d_loss: 0.0280 | g_loss: 1.1108\n",
      "Epoch [    1/   30] | d_loss: 0.0201 | g_loss: 0.9316\n",
      "Epoch [    1/   30] | d_loss: 0.0247 | g_loss: 0.9236\n",
      "Epoch [    1/   30] | d_loss: 0.2563 | g_loss: 1.5985\n",
      "Epoch [    1/   30] | d_loss: 0.4214 | g_loss: 1.3129\n",
      "Epoch [    1/   30] | d_loss: 0.3878 | g_loss: 0.8821\n",
      "Epoch [    1/   30] | d_loss: 0.0083 | g_loss: 1.0541\n",
      "Epoch [    1/   30] | d_loss: 0.0483 | g_loss: 0.9936\n",
      "Epoch [    1/   30] | d_loss: 0.0273 | g_loss: 1.0562\n",
      "Epoch [    1/   30] | d_loss: 0.0127 | g_loss: 1.1054\n",
      "Epoch [    1/   30] | d_loss: 0.0879 | g_loss: 1.2109\n",
      "Epoch [    1/   30] | d_loss: 0.0917 | g_loss: 0.8315\n",
      "Epoch [    1/   30] | d_loss: 0.0365 | g_loss: 0.8830\n",
      "Epoch [    1/   30] | d_loss: 0.0485 | g_loss: 1.2198\n",
      "Epoch [    1/   30] | d_loss: 0.0315 | g_loss: 0.9957\n",
      "Epoch [    1/   30] | d_loss: 0.0133 | g_loss: 0.9631\n",
      "Epoch [    1/   30] | d_loss: 0.0153 | g_loss: 0.9096\n",
      "Epoch [    1/   30] | d_loss: 0.0032 | g_loss: 1.0360\n",
      "Epoch [    1/   30] | d_loss: 0.0579 | g_loss: 0.7856\n",
      "Epoch [    1/   30] | d_loss: 0.0333 | g_loss: 0.9430\n",
      "Epoch [    1/   30] | d_loss: 0.0068 | g_loss: 1.0415\n",
      "Epoch [    1/   30] | d_loss: 0.3700 | g_loss: 1.6408\n",
      "Epoch [    1/   30] | d_loss: 0.0857 | g_loss: 1.0173\n",
      "Epoch [    1/   30] | d_loss: 0.0097 | g_loss: 0.9567\n",
      "Epoch [    1/   30] | d_loss: 0.0382 | g_loss: 1.1081\n",
      "Epoch [    1/   30] | d_loss: 0.0224 | g_loss: 0.8689\n",
      "Epoch [    1/   30] | d_loss: 0.0081 | g_loss: 0.9154\n",
      "Epoch [    2/   30] | d_loss: 0.0019 | g_loss: 0.9471\n",
      "Epoch [    2/   30] | d_loss: 0.0306 | g_loss: 1.0740\n",
      "Epoch [    2/   30] | d_loss: 0.0083 | g_loss: 1.0182\n",
      "Epoch [    2/   30] | d_loss: 0.0309 | g_loss: 0.8880\n",
      "Epoch [    2/   30] | d_loss: 0.0109 | g_loss: 0.9335\n",
      "Epoch [    2/   30] | d_loss: 0.0426 | g_loss: 0.7340\n",
      "Epoch [    2/   30] | d_loss: 0.0123 | g_loss: 0.9244\n",
      "Epoch [    2/   30] | d_loss: 0.0379 | g_loss: 0.8165\n",
      "Epoch [    2/   30] | d_loss: 0.0017 | g_loss: 0.9944\n",
      "Epoch [    2/   30] | d_loss: 0.2333 | g_loss: 1.4110\n",
      "Epoch [    2/   30] | d_loss: 0.0099 | g_loss: 1.0734\n",
      "Epoch [    2/   30] | d_loss: 0.0053 | g_loss: 0.9559\n",
      "Epoch [    2/   30] | d_loss: 0.0065 | g_loss: 0.9474\n",
      "Epoch [    2/   30] | d_loss: 0.0014 | g_loss: 1.0191\n",
      "Epoch [    2/   30] | d_loss: 0.1405 | g_loss: 0.7480\n",
      "Epoch [    2/   30] | d_loss: 0.0155 | g_loss: 1.0697\n",
      "Epoch [    2/   30] | d_loss: 0.0202 | g_loss: 1.1473\n",
      "Epoch [    2/   30] | d_loss: 0.0030 | g_loss: 1.0141\n",
      "Epoch [    2/   30] | d_loss: 0.0202 | g_loss: 1.0068\n",
      "Epoch [    2/   30] | d_loss: 0.0026 | g_loss: 0.9991\n",
      "Epoch [    2/   30] | d_loss: 0.0147 | g_loss: 0.9616\n",
      "Epoch [    2/   30] | d_loss: 0.0053 | g_loss: 1.0438\n",
      "Epoch [    2/   30] | d_loss: 0.0216 | g_loss: 1.0077\n",
      "Epoch [    2/   30] | d_loss: 0.0137 | g_loss: 1.2152\n",
      "Epoch [    2/   30] | d_loss: 0.0007 | g_loss: 1.0066\n",
      "Epoch [    2/   30] | d_loss: 0.0088 | g_loss: 0.9239\n",
      "Epoch [    2/   30] | d_loss: 0.0129 | g_loss: 1.0439\n",
      "Epoch [    2/   30] | d_loss: 0.0100 | g_loss: 1.1062\n",
      "Epoch [    2/   30] | d_loss: 0.0198 | g_loss: 0.9395\n",
      "Epoch [    3/   30] | d_loss: 0.0278 | g_loss: 1.1175\n",
      "Epoch [    3/   30] | d_loss: 0.0013 | g_loss: 1.0167\n",
      "Epoch [    3/   30] | d_loss: 0.0022 | g_loss: 1.0016\n",
      "Epoch [    3/   30] | d_loss: 0.0043 | g_loss: 1.0345\n",
      "Epoch [    3/   30] | d_loss: 0.0123 | g_loss: 1.0349\n",
      "Epoch [    3/   30] | d_loss: 0.0028 | g_loss: 0.9819\n",
      "Epoch [    3/   30] | d_loss: 0.0038 | g_loss: 0.9935\n",
      "Epoch [    3/   30] | d_loss: 0.0018 | g_loss: 1.0238\n",
      "Epoch [    3/   30] | d_loss: 0.0026 | g_loss: 0.9792\n",
      "Epoch [    3/   30] | d_loss: 0.0122 | g_loss: 1.0033\n",
      "Epoch [    3/   30] | d_loss: 0.0199 | g_loss: 0.9370\n",
      "Epoch [    3/   30] | d_loss: 0.0049 | g_loss: 1.0195\n",
      "Epoch [    3/   30] | d_loss: 0.0156 | g_loss: 1.0621\n",
      "Epoch [    3/   30] | d_loss: 0.0221 | g_loss: 1.0238\n",
      "Epoch [    3/   30] | d_loss: 0.0120 | g_loss: 0.9373\n",
      "Epoch [    3/   30] | d_loss: 0.0176 | g_loss: 0.9051\n",
      "Epoch [    3/   30] | d_loss: 0.0013 | g_loss: 0.9811\n",
      "Epoch [    3/   30] | d_loss: 0.0163 | g_loss: 0.9938\n",
      "Epoch [    3/   30] | d_loss: 0.0128 | g_loss: 0.9795\n",
      "Epoch [    3/   30] | d_loss: 0.0355 | g_loss: 1.0540\n",
      "Epoch [    3/   30] | d_loss: 0.0019 | g_loss: 0.9829\n",
      "Epoch [    3/   30] | d_loss: 0.0444 | g_loss: 0.9375\n",
      "Epoch [    3/   30] | d_loss: 0.0192 | g_loss: 1.0447\n",
      "Epoch [    3/   30] | d_loss: 0.0132 | g_loss: 1.0095\n",
      "Epoch [    3/   30] | d_loss: 0.0044 | g_loss: 1.0127\n",
      "Epoch [    3/   30] | d_loss: 0.0045 | g_loss: 1.0034\n",
      "Epoch [    3/   30] | d_loss: 0.0028 | g_loss: 0.9711\n",
      "Epoch [    3/   30] | d_loss: 0.0128 | g_loss: 0.9918\n",
      "Epoch [    3/   30] | d_loss: 0.0547 | g_loss: 1.1615\n",
      "Epoch [    4/   30] | d_loss: 0.0305 | g_loss: 1.0068\n",
      "Epoch [    4/   30] | d_loss: 0.0019 | g_loss: 1.0147\n",
      "Epoch [    4/   30] | d_loss: 0.0092 | g_loss: 1.0558\n",
      "Epoch [    4/   30] | d_loss: 0.0026 | g_loss: 1.0180\n",
      "Epoch [    4/   30] | d_loss: 0.0127 | g_loss: 1.0685\n",
      "Epoch [    4/   30] | d_loss: 0.0095 | g_loss: 1.0712\n",
      "Epoch [    4/   30] | d_loss: 0.0417 | g_loss: 1.0336\n",
      "Epoch [    4/   30] | d_loss: 0.0018 | g_loss: 1.0102\n",
      "Epoch [    4/   30] | d_loss: 0.0102 | g_loss: 0.9254\n",
      "Epoch [    4/   30] | d_loss: 0.0173 | g_loss: 0.8757\n",
      "Epoch [    4/   30] | d_loss: 0.0133 | g_loss: 0.8949\n",
      "Epoch [    4/   30] | d_loss: 0.0017 | g_loss: 0.9900\n",
      "Epoch [    4/   30] | d_loss: 0.0005 | g_loss: 1.0016\n",
      "Epoch [    4/   30] | d_loss: 0.0103 | g_loss: 1.0467\n",
      "Epoch [    4/   30] | d_loss: 0.0099 | g_loss: 1.1004\n",
      "Epoch [    4/   30] | d_loss: 0.0005 | g_loss: 0.9957\n",
      "Epoch [    4/   30] | d_loss: 0.0019 | g_loss: 0.9675\n",
      "Epoch [    4/   30] | d_loss: 0.0022 | g_loss: 0.9747\n",
      "Epoch [    4/   30] | d_loss: 0.0024 | g_loss: 0.9682\n",
      "Epoch [    4/   30] | d_loss: 0.0022 | g_loss: 0.9842\n",
      "Epoch [    4/   30] | d_loss: 0.0085 | g_loss: 1.0649\n",
      "Epoch [    4/   30] | d_loss: 0.0217 | g_loss: 1.0382\n",
      "Epoch [    4/   30] | d_loss: 0.0164 | g_loss: 0.9458\n",
      "Epoch [    4/   30] | d_loss: 0.0820 | g_loss: 1.2842\n",
      "Epoch [    4/   30] | d_loss: 0.0036 | g_loss: 0.9793\n",
      "Epoch [    4/   30] | d_loss: 0.0868 | g_loss: 0.9748\n",
      "Epoch [    4/   30] | d_loss: 0.0431 | g_loss: 1.0746\n",
      "Epoch [    4/   30] | d_loss: 0.0006 | g_loss: 0.9993\n",
      "Epoch [    4/   30] | d_loss: 0.0012 | g_loss: 0.9963\n",
      "Epoch [    5/   30] | d_loss: 0.0019 | g_loss: 1.0199\n",
      "Epoch [    5/   30] | d_loss: 0.0044 | g_loss: 0.9960\n",
      "Epoch [    5/   30] | d_loss: 0.0041 | g_loss: 0.9598\n",
      "Epoch [    5/   30] | d_loss: 0.0021 | g_loss: 1.0012\n",
      "Epoch [    5/   30] | d_loss: 0.0006 | g_loss: 0.9804\n",
      "Epoch [    5/   30] | d_loss: 0.0128 | g_loss: 0.9735\n",
      "Epoch [    5/   30] | d_loss: 0.0083 | g_loss: 0.9347\n",
      "Epoch [    5/   30] | d_loss: 0.0029 | g_loss: 0.9546\n",
      "Epoch [    5/   30] | d_loss: 0.0411 | g_loss: 0.8046\n",
      "Epoch [    5/   30] | d_loss: 0.0009 | g_loss: 1.0005\n",
      "Epoch [    5/   30] | d_loss: 0.0099 | g_loss: 1.0522\n",
      "Epoch [    5/   30] | d_loss: 0.0064 | g_loss: 1.0283\n",
      "Epoch [    5/   30] | d_loss: 0.0008 | g_loss: 1.0129\n",
      "Epoch [    5/   30] | d_loss: 0.0013 | g_loss: 1.0025\n",
      "Epoch [    5/   30] | d_loss: 0.0038 | g_loss: 1.0076\n",
      "Epoch [    5/   30] | d_loss: 0.0089 | g_loss: 1.0076\n",
      "Epoch [    5/   30] | d_loss: 0.0036 | g_loss: 1.0320\n",
      "Epoch [    5/   30] | d_loss: 0.0082 | g_loss: 1.0138\n",
      "Epoch [    5/   30] | d_loss: 0.0047 | g_loss: 1.2240\n",
      "Epoch [    5/   30] | d_loss: 0.0026 | g_loss: 0.9981\n",
      "Epoch [    5/   30] | d_loss: 0.0039 | g_loss: 0.9670\n",
      "Epoch [    5/   30] | d_loss: 0.0067 | g_loss: 0.9639\n",
      "Epoch [    5/   30] | d_loss: 0.0034 | g_loss: 0.9697\n",
      "Epoch [    5/   30] | d_loss: 0.0008 | g_loss: 0.9948\n",
      "Epoch [    5/   30] | d_loss: 0.0009 | g_loss: 1.0135\n",
      "Epoch [    5/   30] | d_loss: 0.0006 | g_loss: 0.9983\n",
      "Epoch [    5/   30] | d_loss: 0.0016 | g_loss: 1.0245\n",
      "Epoch [    5/   30] | d_loss: 0.0006 | g_loss: 1.0022\n",
      "Epoch [    5/   30] | d_loss: 0.0004 | g_loss: 0.9919\n",
      "Epoch [    6/   30] | d_loss: 0.0067 | g_loss: 0.9924\n",
      "Epoch [    6/   30] | d_loss: 0.0031 | g_loss: 1.0502\n",
      "Epoch [    6/   30] | d_loss: 0.0009 | g_loss: 1.0121\n",
      "Epoch [    6/   30] | d_loss: 0.0027 | g_loss: 0.9819\n",
      "Epoch [    6/   30] | d_loss: 0.0029 | g_loss: 0.9661\n",
      "Epoch [    6/   30] | d_loss: 0.0048 | g_loss: 0.9894\n",
      "Epoch [    6/   30] | d_loss: 0.0237 | g_loss: 1.1518\n",
      "Epoch [    6/   30] | d_loss: 0.0472 | g_loss: 1.6281\n",
      "Epoch [    6/   30] | d_loss: 0.0017 | g_loss: 0.9690\n",
      "Epoch [    6/   30] | d_loss: 0.0019 | g_loss: 1.0204\n",
      "Epoch [    6/   30] | d_loss: 0.0033 | g_loss: 0.9632\n",
      "Epoch [    6/   30] | d_loss: 0.0011 | g_loss: 1.0249\n",
      "Epoch [    6/   30] | d_loss: 0.0007 | g_loss: 1.0073\n",
      "Epoch [    6/   30] | d_loss: 0.0016 | g_loss: 0.9840\n",
      "Epoch [    6/   30] | d_loss: 0.0032 | g_loss: 1.0154\n",
      "Epoch [    6/   30] | d_loss: 0.0005 | g_loss: 0.9909\n",
      "Epoch [    6/   30] | d_loss: 0.0005 | g_loss: 1.0017\n",
      "Epoch [    6/   30] | d_loss: 0.0021 | g_loss: 1.0293\n",
      "Epoch [    6/   30] | d_loss: 0.0007 | g_loss: 0.9920\n",
      "Epoch [    6/   30] | d_loss: 0.0023 | g_loss: 0.9686\n",
      "Epoch [    6/   30] | d_loss: 0.0004 | g_loss: 1.0084\n",
      "Epoch [    6/   30] | d_loss: 0.0014 | g_loss: 0.9813\n",
      "Epoch [    6/   30] | d_loss: 0.0070 | g_loss: 1.0775\n",
      "Epoch [    6/   30] | d_loss: 0.0031 | g_loss: 0.9673\n",
      "Epoch [    6/   30] | d_loss: 0.0013 | g_loss: 1.0288\n",
      "Epoch [    6/   30] | d_loss: 0.0024 | g_loss: 1.0129\n",
      "Epoch [    6/   30] | d_loss: 0.0076 | g_loss: 1.2777\n",
      "Epoch [    6/   30] | d_loss: 0.0017 | g_loss: 1.0198\n",
      "Epoch [    6/   30] | d_loss: 0.0004 | g_loss: 1.0010\n",
      "Epoch [    7/   30] | d_loss: 0.0299 | g_loss: 1.1475\n",
      "Epoch [    7/   30] | d_loss: 0.0133 | g_loss: 0.9621\n",
      "Epoch [    7/   30] | d_loss: 0.0067 | g_loss: 1.2529\n",
      "Epoch [    7/   30] | d_loss: 0.0004 | g_loss: 0.9967\n",
      "Epoch [    7/   30] | d_loss: 0.0017 | g_loss: 0.9727\n",
      "Epoch [    7/   30] | d_loss: 0.0003 | g_loss: 1.0087\n",
      "Epoch [    7/   30] | d_loss: 0.0003 | g_loss: 1.0051\n",
      "Epoch [    7/   30] | d_loss: 0.0005 | g_loss: 0.9823\n",
      "Epoch [    7/   30] | d_loss: 0.0011 | g_loss: 0.9800\n",
      "Epoch [    7/   30] | d_loss: 0.0065 | g_loss: 0.9407\n",
      "Epoch [    7/   30] | d_loss: 0.0036 | g_loss: 1.0234\n",
      "Epoch [    7/   30] | d_loss: 0.0012 | g_loss: 0.9825\n",
      "Epoch [    7/   30] | d_loss: 0.0004 | g_loss: 0.9889\n",
      "Epoch [    7/   30] | d_loss: 0.0011 | g_loss: 1.0121\n",
      "Epoch [    7/   30] | d_loss: 0.0003 | g_loss: 0.9990\n",
      "Epoch [    7/   30] | d_loss: 0.0003 | g_loss: 1.0149\n",
      "Epoch [    7/   30] | d_loss: 0.0008 | g_loss: 0.9921\n",
      "Epoch [    7/   30] | d_loss: 0.0005 | g_loss: 1.0103\n",
      "Epoch [    7/   30] | d_loss: 0.0023 | g_loss: 0.9997\n",
      "Epoch [    7/   30] | d_loss: 0.0005 | g_loss: 0.9972\n",
      "Epoch [    7/   30] | d_loss: 0.0064 | g_loss: 0.9990\n",
      "Epoch [    7/   30] | d_loss: 0.0056 | g_loss: 1.0338\n",
      "Epoch [    7/   30] | d_loss: 0.0003 | g_loss: 1.0019\n",
      "Epoch [    7/   30] | d_loss: 0.0046 | g_loss: 1.0760\n",
      "Epoch [    7/   30] | d_loss: 0.0007 | g_loss: 0.9804\n",
      "Epoch [    7/   30] | d_loss: 0.0072 | g_loss: 1.0816\n",
      "Epoch [    7/   30] | d_loss: 0.0006 | g_loss: 0.9860\n",
      "Epoch [    7/   30] | d_loss: 0.0004 | g_loss: 1.0125\n",
      "Epoch [    7/   30] | d_loss: 0.0069 | g_loss: 1.0129\n",
      "Epoch [    8/   30] | d_loss: 0.0084 | g_loss: 1.0889\n",
      "Epoch [    8/   30] | d_loss: 0.0048 | g_loss: 1.0401\n",
      "Epoch [    8/   30] | d_loss: 0.0044 | g_loss: 1.0585\n",
      "Epoch [    8/   30] | d_loss: 0.0003 | g_loss: 0.9890\n",
      "Epoch [    8/   30] | d_loss: 0.0003 | g_loss: 0.9898\n",
      "Epoch [    8/   30] | d_loss: 0.0034 | g_loss: 0.9829\n",
      "Epoch [    8/   30] | d_loss: 0.0112 | g_loss: 1.0620\n",
      "Epoch [    8/   30] | d_loss: 0.1523 | g_loss: 1.9733\n",
      "Epoch [    8/   30] | d_loss: 0.0014 | g_loss: 0.9753\n",
      "Epoch [    8/   30] | d_loss: 0.0172 | g_loss: 0.7904\n",
      "Epoch [    8/   30] | d_loss: 0.0025 | g_loss: 1.0969\n",
      "Epoch [    8/   30] | d_loss: 0.0006 | g_loss: 0.9981\n",
      "Epoch [    8/   30] | d_loss: 0.0050 | g_loss: 0.8845\n",
      "Epoch [    8/   30] | d_loss: 0.0030 | g_loss: 0.9513\n",
      "Epoch [    8/   30] | d_loss: 0.0025 | g_loss: 0.9453\n",
      "Epoch [    8/   30] | d_loss: 0.0005 | g_loss: 0.9891\n",
      "Epoch [    8/   30] | d_loss: 0.0005 | g_loss: 1.0181\n",
      "Epoch [    8/   30] | d_loss: 0.0003 | g_loss: 0.9968\n",
      "Epoch [    8/   30] | d_loss: 0.0023 | g_loss: 0.9034\n",
      "Epoch [    8/   30] | d_loss: 0.0006 | g_loss: 0.9820\n",
      "Epoch [    8/   30] | d_loss: 0.0005 | g_loss: 1.0214\n",
      "Epoch [    8/   30] | d_loss: 0.0005 | g_loss: 1.0057\n",
      "Epoch [    8/   30] | d_loss: 0.0004 | g_loss: 0.9801\n",
      "Epoch [    8/   30] | d_loss: 0.0002 | g_loss: 0.9974\n",
      "Epoch [    8/   30] | d_loss: 0.0023 | g_loss: 1.0423\n",
      "Epoch [    8/   30] | d_loss: 0.0003 | g_loss: 0.9939\n",
      "Epoch [    8/   30] | d_loss: 0.0009 | g_loss: 1.0173\n",
      "Epoch [    8/   30] | d_loss: 0.0003 | g_loss: 1.0034\n",
      "Epoch [    8/   30] | d_loss: 0.0004 | g_loss: 1.0053\n",
      "Epoch [    9/   30] | d_loss: 0.0004 | g_loss: 1.0076\n",
      "Epoch [    9/   30] | d_loss: 0.0015 | g_loss: 1.0252\n",
      "Epoch [    9/   30] | d_loss: 0.0002 | g_loss: 0.9955\n",
      "Epoch [    9/   30] | d_loss: 0.0013 | g_loss: 0.9792\n",
      "Epoch [    9/   30] | d_loss: 0.0012 | g_loss: 0.9814\n",
      "Epoch [    9/   30] | d_loss: 0.0001 | g_loss: 0.9985\n",
      "Epoch [    9/   30] | d_loss: 0.0002 | g_loss: 1.0065\n",
      "Epoch [    9/   30] | d_loss: 0.0001 | g_loss: 1.0043\n",
      "Epoch [    9/   30] | d_loss: 0.0007 | g_loss: 0.9839\n",
      "Epoch [    9/   30] | d_loss: 0.0003 | g_loss: 0.9903\n",
      "Epoch [    9/   30] | d_loss: 0.0028 | g_loss: 0.9688\n",
      "Epoch [    9/   30] | d_loss: 0.0003 | g_loss: 1.0078\n",
      "Epoch [    9/   30] | d_loss: 0.0002 | g_loss: 1.0027\n",
      "Epoch [    9/   30] | d_loss: 0.0001 | g_loss: 1.0004\n",
      "Epoch [    9/   30] | d_loss: 0.0028 | g_loss: 0.9504\n",
      "Epoch [    9/   30] | d_loss: 0.0031 | g_loss: 0.9610\n",
      "Epoch [    9/   30] | d_loss: 0.0010 | g_loss: 0.9801\n",
      "Epoch [    9/   30] | d_loss: 0.0067 | g_loss: 1.0504\n",
      "Epoch [    9/   30] | d_loss: 0.0002 | g_loss: 1.0016\n",
      "Epoch [    9/   30] | d_loss: 0.0013 | g_loss: 0.9913\n",
      "Epoch [    9/   30] | d_loss: 0.0010 | g_loss: 1.0021\n",
      "Epoch [    9/   30] | d_loss: 0.0003 | g_loss: 1.0032\n",
      "Epoch [    9/   30] | d_loss: 0.0007 | g_loss: 1.0054\n",
      "Epoch [    9/   30] | d_loss: 0.0002 | g_loss: 0.9994\n",
      "Epoch [    9/   30] | d_loss: 0.0001 | g_loss: 1.0046\n",
      "Epoch [    9/   30] | d_loss: 0.0009 | g_loss: 0.9897\n",
      "Epoch [    9/   30] | d_loss: 0.0006 | g_loss: 0.9943\n",
      "Epoch [    9/   30] | d_loss: 0.0007 | g_loss: 0.9985\n",
      "Epoch [    9/   30] | d_loss: 0.0002 | g_loss: 1.0064\n",
      "Epoch [   10/   30] | d_loss: 0.0097 | g_loss: 0.9602\n",
      "Epoch [   10/   30] | d_loss: 0.0002 | g_loss: 1.0047\n",
      "Epoch [   10/   30] | d_loss: 0.0008 | g_loss: 1.0111\n",
      "Epoch [   10/   30] | d_loss: 0.0006 | g_loss: 1.0011\n",
      "Epoch [   10/   30] | d_loss: 0.0011 | g_loss: 1.0187\n",
      "Epoch [   10/   30] | d_loss: 0.0001 | g_loss: 0.9978\n",
      "Epoch [   10/   30] | d_loss: 0.0012 | g_loss: 0.9780\n",
      "Epoch [   10/   30] | d_loss: 0.0005 | g_loss: 1.0065\n",
      "Epoch [   10/   30] | d_loss: 0.0001 | g_loss: 0.9978\n",
      "Epoch [   10/   30] | d_loss: 0.0010 | g_loss: 0.9861\n",
      "Epoch [   10/   30] | d_loss: 0.0016 | g_loss: 1.0188\n",
      "Epoch [   10/   30] | d_loss: 0.0006 | g_loss: 0.9941\n",
      "Epoch [   10/   30] | d_loss: 0.0006 | g_loss: 1.0035\n",
      "Epoch [   10/   30] | d_loss: 0.0008 | g_loss: 0.9762\n",
      "Epoch [   10/   30] | d_loss: 0.0023 | g_loss: 0.9651\n",
      "Epoch [   10/   30] | d_loss: 0.0001 | g_loss: 1.0029\n",
      "Epoch [   10/   30] | d_loss: 0.0010 | g_loss: 0.9885\n",
      "Epoch [   10/   30] | d_loss: 0.0003 | g_loss: 1.0044\n",
      "Epoch [   10/   30] | d_loss: 0.0003 | g_loss: 1.0012\n",
      "Epoch [   10/   30] | d_loss: 0.0037 | g_loss: 1.0104\n",
      "Epoch [   10/   30] | d_loss: 0.0005 | g_loss: 0.9975\n",
      "Epoch [   10/   30] | d_loss: 0.0001 | g_loss: 0.9952\n",
      "Epoch [   10/   30] | d_loss: 0.0001 | g_loss: 1.0021\n",
      "Epoch [   10/   30] | d_loss: 0.0002 | g_loss: 0.9962\n",
      "Epoch [   10/   30] | d_loss: 0.0002 | g_loss: 0.9955\n",
      "Epoch [   10/   30] | d_loss: 0.0002 | g_loss: 0.9972\n",
      "Epoch [   10/   30] | d_loss: 0.0003 | g_loss: 1.0092\n",
      "Epoch [   10/   30] | d_loss: 0.0005 | g_loss: 1.0064\n",
      "Epoch [   10/   30] | d_loss: 0.0005 | g_loss: 0.9951\n",
      "Epoch [   11/   30] | d_loss: 0.0032 | g_loss: 0.9637\n",
      "Epoch [   11/   30] | d_loss: 0.0007 | g_loss: 1.0177\n",
      "Epoch [   11/   30] | d_loss: 0.0002 | g_loss: 0.9966\n",
      "Epoch [   11/   30] | d_loss: 0.0020 | g_loss: 0.9684\n",
      "Epoch [   11/   30] | d_loss: 0.0004 | g_loss: 0.9911\n",
      "Epoch [   11/   30] | d_loss: 0.0005 | g_loss: 1.0219\n",
      "Epoch [   11/   30] | d_loss: 0.0020 | g_loss: 1.0624\n",
      "Epoch [   11/   30] | d_loss: 0.0003 | g_loss: 0.9816\n",
      "Epoch [   11/   30] | d_loss: 0.0007 | g_loss: 1.0184\n",
      "Epoch [   11/   30] | d_loss: 0.0006 | g_loss: 1.0068\n",
      "Epoch [   11/   30] | d_loss: 0.0001 | g_loss: 1.0052\n",
      "Epoch [   11/   30] | d_loss: 0.0007 | g_loss: 0.9792\n",
      "Epoch [   11/   30] | d_loss: 0.0006 | g_loss: 0.9876\n",
      "Epoch [   11/   30] | d_loss: 0.0001 | g_loss: 1.0004\n",
      "Epoch [   11/   30] | d_loss: 0.0002 | g_loss: 1.0061\n",
      "Epoch [   11/   30] | d_loss: 0.0042 | g_loss: 1.0502\n",
      "Epoch [   11/   30] | d_loss: 0.0001 | g_loss: 0.9998\n",
      "Epoch [   11/   30] | d_loss: 0.0007 | g_loss: 0.9867\n",
      "Epoch [   11/   30] | d_loss: 0.0001 | g_loss: 1.0051\n",
      "Epoch [   11/   30] | d_loss: 0.0019 | g_loss: 1.0030\n",
      "Epoch [   11/   30] | d_loss: 0.0009 | g_loss: 0.9686\n",
      "Epoch [   11/   30] | d_loss: 0.0002 | g_loss: 0.9999\n",
      "Epoch [   11/   30] | d_loss: 0.0007 | g_loss: 1.0083\n",
      "Epoch [   11/   30] | d_loss: 0.0002 | g_loss: 1.0077\n",
      "Epoch [   11/   30] | d_loss: 0.0003 | g_loss: 1.0108\n",
      "Epoch [   11/   30] | d_loss: 0.0007 | g_loss: 1.0110\n",
      "Epoch [   11/   30] | d_loss: 0.0006 | g_loss: 0.9865\n",
      "Epoch [   11/   30] | d_loss: 0.0002 | g_loss: 1.0114\n",
      "Epoch [   11/   30] | d_loss: 0.0012 | g_loss: 0.9759\n",
      "Epoch [   12/   30] | d_loss: 0.0007 | g_loss: 0.9890\n",
      "Epoch [   12/   30] | d_loss: 0.0001 | g_loss: 1.0053\n",
      "Epoch [   12/   30] | d_loss: 0.0003 | g_loss: 1.0564\n",
      "Epoch [   12/   30] | d_loss: 0.0031 | g_loss: 0.9713\n",
      "Epoch [   12/   30] | d_loss: 0.0034 | g_loss: 1.0209\n",
      "Epoch [   12/   30] | d_loss: 0.0026 | g_loss: 0.9882\n",
      "Epoch [   12/   30] | d_loss: 0.0001 | g_loss: 1.0064\n",
      "Epoch [   12/   30] | d_loss: 0.0007 | g_loss: 0.9926\n",
      "Epoch [   12/   30] | d_loss: 0.0004 | g_loss: 1.0088\n",
      "Epoch [   12/   30] | d_loss: 0.0004 | g_loss: 0.9871\n",
      "Epoch [   12/   30] | d_loss: 3.0019 | g_loss: 0.5575\n",
      "Epoch [   12/   30] | d_loss: 0.0045 | g_loss: 0.8626\n",
      "Epoch [   12/   30] | d_loss: 0.0006 | g_loss: 1.0108\n",
      "Epoch [   12/   30] | d_loss: 0.0006 | g_loss: 0.9817\n",
      "Epoch [   12/   30] | d_loss: 0.0008 | g_loss: 0.9973\n",
      "Epoch [   12/   30] | d_loss: 0.0018 | g_loss: 1.0614\n",
      "Epoch [   12/   30] | d_loss: 0.0020 | g_loss: 1.0539\n",
      "Epoch [   12/   30] | d_loss: 0.0004 | g_loss: 1.0104\n",
      "Epoch [   12/   30] | d_loss: 0.0003 | g_loss: 1.0102\n",
      "Epoch [   12/   30] | d_loss: 0.0007 | g_loss: 0.9886\n",
      "Epoch [   12/   30] | d_loss: 0.0003 | g_loss: 1.0024\n",
      "Epoch [   12/   30] | d_loss: 0.0001 | g_loss: 1.0004\n",
      "Epoch [   12/   30] | d_loss: 0.0001 | g_loss: 1.0173\n",
      "Epoch [   12/   30] | d_loss: 0.0026 | g_loss: 0.9582\n",
      "Epoch [   12/   30] | d_loss: 0.0002 | g_loss: 0.9982\n",
      "Epoch [   12/   30] | d_loss: 0.0004 | g_loss: 1.0037\n",
      "Epoch [   12/   30] | d_loss: 0.0154 | g_loss: 0.7818\n",
      "Epoch [   12/   30] | d_loss: 0.0004 | g_loss: 0.9944\n",
      "Epoch [   12/   30] | d_loss: 0.0001 | g_loss: 1.0016\n",
      "Epoch [   13/   30] | d_loss: 0.0026 | g_loss: 0.9836\n",
      "Epoch [   13/   30] | d_loss: 0.0003 | g_loss: 0.9936\n",
      "Epoch [   13/   30] | d_loss: 0.0002 | g_loss: 0.9886\n",
      "Epoch [   13/   30] | d_loss: 0.0004 | g_loss: 0.9927\n",
      "Epoch [   13/   30] | d_loss: 0.0001 | g_loss: 1.0009\n",
      "Epoch [   13/   30] | d_loss: 0.0005 | g_loss: 0.9857\n",
      "Epoch [   13/   30] | d_loss: 0.0001 | g_loss: 1.0017\n",
      "Epoch [   13/   30] | d_loss: 0.0001 | g_loss: 0.9980\n",
      "Epoch [   13/   30] | d_loss: 0.0002 | g_loss: 1.0005\n",
      "Epoch [   13/   30] | d_loss: 0.0003 | g_loss: 0.9962\n",
      "Epoch [   13/   30] | d_loss: 0.0003 | g_loss: 0.9966\n",
      "Epoch [   13/   30] | d_loss: 0.0001 | g_loss: 0.9994\n",
      "Epoch [   13/   30] | d_loss: 0.0001 | g_loss: 1.0008\n",
      "Epoch [   13/   30] | d_loss: 0.0001 | g_loss: 0.9991\n",
      "Epoch [   13/   30] | d_loss: 0.0001 | g_loss: 0.9999\n",
      "Epoch [   13/   30] | d_loss: 0.0002 | g_loss: 1.0055\n",
      "Epoch [   13/   30] | d_loss: 0.0001 | g_loss: 1.0012\n",
      "Epoch [   13/   30] | d_loss: 0.0008 | g_loss: 0.9631\n",
      "Epoch [   13/   30] | d_loss: 0.0074 | g_loss: 1.2008\n",
      "Epoch [   13/   30] | d_loss: 0.0001 | g_loss: 0.9993\n",
      "Epoch [   13/   30] | d_loss: 0.0001 | g_loss: 0.9936\n",
      "Epoch [   13/   30] | d_loss: 0.0002 | g_loss: 1.0199\n",
      "Epoch [   13/   30] | d_loss: 0.0001 | g_loss: 0.9839\n",
      "Epoch [   13/   30] | d_loss: 0.0001 | g_loss: 1.0002\n",
      "Epoch [   13/   30] | d_loss: 0.0002 | g_loss: 1.0154\n",
      "Epoch [   13/   30] | d_loss: 0.0001 | g_loss: 0.9964\n",
      "Epoch [   13/   30] | d_loss: 0.0000 | g_loss: 0.9958\n",
      "Epoch [   13/   30] | d_loss: 0.0000 | g_loss: 1.0012\n",
      "Epoch [   13/   30] | d_loss: 0.0001 | g_loss: 0.9928\n",
      "Epoch [   14/   30] | d_loss: 0.0006 | g_loss: 1.0040\n",
      "Epoch [   14/   30] | d_loss: 0.0001 | g_loss: 0.9891\n",
      "Epoch [   14/   30] | d_loss: 0.0001 | g_loss: 1.0074\n",
      "Epoch [   14/   30] | d_loss: 0.0002 | g_loss: 0.9814\n",
      "Epoch [   14/   30] | d_loss: 0.0003 | g_loss: 1.0139\n",
      "Epoch [   14/   30] | d_loss: 0.0001 | g_loss: 0.9852\n",
      "Epoch [   14/   30] | d_loss: 0.0001 | g_loss: 1.0046\n",
      "Epoch [   14/   30] | d_loss: 0.0001 | g_loss: 0.9898\n",
      "Epoch [   14/   30] | d_loss: 0.0001 | g_loss: 1.0000\n",
      "Epoch [   14/   30] | d_loss: 0.0003 | g_loss: 0.9817\n",
      "Epoch [   14/   30] | d_loss: 0.0001 | g_loss: 0.9957\n",
      "Epoch [   14/   30] | d_loss: 0.0001 | g_loss: 0.9889\n",
      "Epoch [   14/   30] | d_loss: 0.0001 | g_loss: 0.9999\n",
      "Epoch [   14/   30] | d_loss: 0.0000 | g_loss: 0.9987\n",
      "Epoch [   14/   30] | d_loss: 0.0001 | g_loss: 0.9976\n",
      "Epoch [   14/   30] | d_loss: 0.0000 | g_loss: 0.9969\n",
      "Epoch [   14/   30] | d_loss: 0.0000 | g_loss: 0.9962\n",
      "Epoch [   14/   30] | d_loss: 0.0003 | g_loss: 1.0150\n",
      "Epoch [   14/   30] | d_loss: 0.0005 | g_loss: 1.0131\n",
      "Epoch [   14/   30] | d_loss: 0.0002 | g_loss: 1.0017\n",
      "Epoch [   14/   30] | d_loss: 0.0001 | g_loss: 0.9998\n",
      "Epoch [   14/   30] | d_loss: 0.0002 | g_loss: 0.9898\n",
      "Epoch [   14/   30] | d_loss: 0.0004 | g_loss: 0.9836\n",
      "Epoch [   14/   30] | d_loss: 0.0002 | g_loss: 0.9944\n",
      "Epoch [   14/   30] | d_loss: 0.0001 | g_loss: 0.9946\n",
      "Epoch [   14/   30] | d_loss: 0.0002 | g_loss: 1.0062\n",
      "Epoch [   14/   30] | d_loss: 0.0001 | g_loss: 1.0073\n",
      "Epoch [   14/   30] | d_loss: 0.0001 | g_loss: 0.9914\n",
      "Epoch [   14/   30] | d_loss: 0.0007 | g_loss: 1.0239\n",
      "Epoch [   15/   30] | d_loss: 0.0001 | g_loss: 0.9950\n",
      "Epoch [   15/   30] | d_loss: 0.0001 | g_loss: 1.0094\n",
      "Epoch [   15/   30] | d_loss: 0.0001 | g_loss: 1.0081\n",
      "Epoch [   15/   30] | d_loss: 0.0000 | g_loss: 0.9978\n",
      "Epoch [   15/   30] | d_loss: 0.0001 | g_loss: 1.0020\n",
      "Epoch [   15/   30] | d_loss: 0.0003 | g_loss: 0.9871\n",
      "Epoch [   15/   30] | d_loss: 0.0002 | g_loss: 1.0046\n",
      "Epoch [   15/   30] | d_loss: 0.0003 | g_loss: 0.9867\n",
      "Epoch [   15/   30] | d_loss: 0.0001 | g_loss: 0.9935\n",
      "Epoch [   15/   30] | d_loss: 0.0001 | g_loss: 1.0055\n",
      "Epoch [   15/   30] | d_loss: 0.0001 | g_loss: 0.9949\n",
      "Epoch [   15/   30] | d_loss: 0.0003 | g_loss: 1.0115\n",
      "Epoch [   15/   30] | d_loss: 0.0001 | g_loss: 0.9957\n",
      "Epoch [   15/   30] | d_loss: 0.0001 | g_loss: 1.0056\n",
      "Epoch [   15/   30] | d_loss: 0.0006 | g_loss: 1.0126\n",
      "Epoch [   15/   30] | d_loss: 0.0001 | g_loss: 1.0063\n",
      "Epoch [   15/   30] | d_loss: 0.0001 | g_loss: 0.9969\n",
      "Epoch [   15/   30] | d_loss: 0.0000 | g_loss: 1.0000\n",
      "Epoch [   15/   30] | d_loss: 0.0002 | g_loss: 1.0111\n",
      "Epoch [   15/   30] | d_loss: 0.0006 | g_loss: 1.0174\n",
      "Epoch [   15/   30] | d_loss: 0.0001 | g_loss: 1.0033\n",
      "Epoch [   15/   30] | d_loss: 0.0002 | g_loss: 0.9912\n",
      "Epoch [   15/   30] | d_loss: 0.0001 | g_loss: 1.0006\n",
      "Epoch [   15/   30] | d_loss: 0.0001 | g_loss: 1.0054\n",
      "Epoch [   15/   30] | d_loss: 0.0004 | g_loss: 0.9843\n",
      "Epoch [   15/   30] | d_loss: 0.0004 | g_loss: 0.9886\n",
      "Epoch [   15/   30] | d_loss: 0.0005 | g_loss: 0.9874\n",
      "Epoch [   15/   30] | d_loss: 0.0006 | g_loss: 1.0121\n",
      "Epoch [   15/   30] | d_loss: 0.0008 | g_loss: 1.0185\n",
      "Epoch [   16/   30] | d_loss: 0.0011 | g_loss: 1.0190\n",
      "Epoch [   16/   30] | d_loss: 0.0007 | g_loss: 1.0134\n",
      "Epoch [   16/   30] | d_loss: 0.0008 | g_loss: 1.0172\n",
      "Epoch [   16/   30] | d_loss: 0.0000 | g_loss: 1.0012\n",
      "Epoch [   16/   30] | d_loss: 0.0001 | g_loss: 0.9964\n",
      "Epoch [   16/   30] | d_loss: 0.0005 | g_loss: 0.9901\n",
      "Epoch [   16/   30] | d_loss: 0.0001 | g_loss: 0.9980\n",
      "Epoch [   16/   30] | d_loss: 0.0003 | g_loss: 0.9860\n",
      "Epoch [   16/   30] | d_loss: 0.0010 | g_loss: 1.0129\n",
      "Epoch [   16/   30] | d_loss: 0.0003 | g_loss: 0.9977\n",
      "Epoch [   16/   30] | d_loss: 0.0001 | g_loss: 1.0045\n",
      "Epoch [   16/   30] | d_loss: 0.0001 | g_loss: 1.0060\n",
      "Epoch [   16/   30] | d_loss: 0.0010 | g_loss: 0.9794\n",
      "Epoch [   16/   30] | d_loss: 0.0002 | g_loss: 0.9927\n",
      "Epoch [   16/   30] | d_loss: 0.0005 | g_loss: 1.0123\n",
      "Epoch [   16/   30] | d_loss: 0.0000 | g_loss: 0.9975\n",
      "Epoch [   16/   30] | d_loss: 0.0012 | g_loss: 0.9801\n",
      "Epoch [   16/   30] | d_loss: 0.0005 | g_loss: 1.0184\n",
      "Epoch [   16/   30] | d_loss: 0.0007 | g_loss: 0.9815\n",
      "Epoch [   16/   30] | d_loss: 0.0052 | g_loss: 1.0501\n",
      "Epoch [   16/   30] | d_loss: 0.0001 | g_loss: 1.0069\n",
      "Epoch [   16/   30] | d_loss: 0.0001 | g_loss: 1.0005\n",
      "Epoch [   16/   30] | d_loss: 0.0001 | g_loss: 1.0025\n",
      "Epoch [   16/   30] | d_loss: 0.0001 | g_loss: 0.9933\n",
      "Epoch [   16/   30] | d_loss: 0.0001 | g_loss: 0.9959\n",
      "Epoch [   16/   30] | d_loss: 0.0002 | g_loss: 0.9992\n",
      "Epoch [   16/   30] | d_loss: 0.0000 | g_loss: 0.9974\n",
      "Epoch [   16/   30] | d_loss: 0.0001 | g_loss: 1.0023\n",
      "Epoch [   16/   30] | d_loss: 0.0005 | g_loss: 0.9850\n",
      "Epoch [   17/   30] | d_loss: 0.0019 | g_loss: 1.0095\n",
      "Epoch [   17/   30] | d_loss: 0.0012 | g_loss: 0.9995\n",
      "Epoch [   17/   30] | d_loss: 0.0005 | g_loss: 0.9925\n",
      "Epoch [   17/   30] | d_loss: 0.0001 | g_loss: 0.9967\n",
      "Epoch [   17/   30] | d_loss: 0.0001 | g_loss: 0.9947\n",
      "Epoch [   17/   30] | d_loss: 0.0001 | g_loss: 1.0020\n",
      "Epoch [   17/   30] | d_loss: 0.0003 | g_loss: 1.0069\n",
      "Epoch [   17/   30] | d_loss: 0.0002 | g_loss: 1.0045\n",
      "Epoch [   17/   30] | d_loss: 0.0003 | g_loss: 0.9929\n",
      "Epoch [   17/   30] | d_loss: 0.0004 | g_loss: 1.0093\n",
      "Epoch [   17/   30] | d_loss: 0.0001 | g_loss: 0.9971\n",
      "Epoch [   17/   30] | d_loss: 0.0001 | g_loss: 0.9957\n",
      "Epoch [   17/   30] | d_loss: 0.0000 | g_loss: 1.0005\n",
      "Epoch [   17/   30] | d_loss: 0.0001 | g_loss: 1.0096\n",
      "Epoch [   17/   30] | d_loss: 0.0003 | g_loss: 1.0018\n",
      "Epoch [   17/   30] | d_loss: 0.0002 | g_loss: 0.9964\n",
      "Epoch [   17/   30] | d_loss: 0.0002 | g_loss: 1.0028\n",
      "Epoch [   17/   30] | d_loss: 0.0008 | g_loss: 0.9888\n",
      "Epoch [   17/   30] | d_loss: 0.0002 | g_loss: 0.9935\n",
      "Epoch [   17/   30] | d_loss: 0.0000 | g_loss: 0.9983\n",
      "Epoch [   17/   30] | d_loss: 0.0002 | g_loss: 1.0004\n",
      "Epoch [   17/   30] | d_loss: 0.0005 | g_loss: 1.0021\n",
      "Epoch [   17/   30] | d_loss: 0.0001 | g_loss: 1.0036\n",
      "Epoch [   17/   30] | d_loss: 0.0001 | g_loss: 1.0048\n",
      "Epoch [   17/   30] | d_loss: 0.0000 | g_loss: 1.0039\n",
      "Epoch [   17/   30] | d_loss: 0.0009 | g_loss: 1.0132\n",
      "Epoch [   17/   30] | d_loss: 0.0001 | g_loss: 1.0023\n",
      "Epoch [   17/   30] | d_loss: 0.0003 | g_loss: 0.9887\n",
      "Epoch [   17/   30] | d_loss: 0.0001 | g_loss: 0.9975\n",
      "Epoch [   18/   30] | d_loss: 0.0001 | g_loss: 1.0028\n",
      "Epoch [   18/   30] | d_loss: 0.0001 | g_loss: 0.9942\n",
      "Epoch [   18/   30] | d_loss: 0.0006 | g_loss: 0.9852\n",
      "Epoch [   18/   30] | d_loss: 0.0001 | g_loss: 0.9993\n",
      "Epoch [   18/   30] | d_loss: 0.0004 | g_loss: 1.0087\n",
      "Epoch [   18/   30] | d_loss: 0.0185 | g_loss: 0.9156\n",
      "Epoch [   18/   30] | d_loss: 0.0041 | g_loss: 0.9175\n",
      "Epoch [   18/   30] | d_loss: 0.0045 | g_loss: 1.1080\n",
      "Epoch [   18/   30] | d_loss: 0.0008 | g_loss: 1.0102\n",
      "Epoch [   18/   30] | d_loss: 0.0037 | g_loss: 1.0557\n",
      "Epoch [   18/   30] | d_loss: 0.0011 | g_loss: 1.0085\n",
      "Epoch [   18/   30] | d_loss: 0.0035 | g_loss: 0.9164\n",
      "Epoch [   18/   30] | d_loss: 0.0047 | g_loss: 0.9420\n",
      "Epoch [   18/   30] | d_loss: 0.0008 | g_loss: 0.9890\n",
      "Epoch [   18/   30] | d_loss: 0.0010 | g_loss: 1.0203\n",
      "Epoch [   18/   30] | d_loss: 0.0005 | g_loss: 0.9961\n",
      "Epoch [   18/   30] | d_loss: 0.0006 | g_loss: 0.9717\n",
      "Epoch [   18/   30] | d_loss: 0.0002 | g_loss: 1.0165\n",
      "Epoch [   18/   30] | d_loss: 0.0004 | g_loss: 0.9823\n",
      "Epoch [   18/   30] | d_loss: 0.0007 | g_loss: 1.0434\n",
      "Epoch [   18/   30] | d_loss: 0.0152 | g_loss: 0.8765\n",
      "Epoch [   18/   30] | d_loss: 0.0008 | g_loss: 0.9730\n",
      "Epoch [   18/   30] | d_loss: 0.0005 | g_loss: 1.0241\n",
      "Epoch [   18/   30] | d_loss: 0.0004 | g_loss: 1.0082\n",
      "Epoch [   18/   30] | d_loss: 0.0006 | g_loss: 0.9822\n",
      "Epoch [   18/   30] | d_loss: 0.0005 | g_loss: 0.9888\n",
      "Epoch [   18/   30] | d_loss: 0.0005 | g_loss: 1.0492\n",
      "Epoch [   18/   30] | d_loss: 0.0007 | g_loss: 0.9657\n",
      "Epoch [   18/   30] | d_loss: 0.0010 | g_loss: 0.9650\n",
      "Epoch [   19/   30] | d_loss: 0.0023 | g_loss: 1.0223\n",
      "Epoch [   19/   30] | d_loss: 0.0005 | g_loss: 1.0111\n",
      "Epoch [   19/   30] | d_loss: 0.0003 | g_loss: 1.0129\n",
      "Epoch [   19/   30] | d_loss: 0.0008 | g_loss: 0.9646\n",
      "Epoch [   19/   30] | d_loss: 0.0007 | g_loss: 0.9862\n",
      "Epoch [   19/   30] | d_loss: 0.0006 | g_loss: 0.9804\n",
      "Epoch [   19/   30] | d_loss: 0.0026 | g_loss: 1.0718\n",
      "Epoch [   19/   30] | d_loss: 0.0041 | g_loss: 0.9156\n",
      "Epoch [   19/   30] | d_loss: 0.0001 | g_loss: 1.0082\n",
      "Epoch [   19/   30] | d_loss: 0.0003 | g_loss: 1.0210\n",
      "Epoch [   19/   30] | d_loss: 0.0009 | g_loss: 1.0118\n",
      "Epoch [   19/   30] | d_loss: 0.0018 | g_loss: 0.9371\n",
      "Epoch [   19/   30] | d_loss: 0.0016 | g_loss: 0.9511\n",
      "Epoch [   19/   30] | d_loss: 0.0007 | g_loss: 1.0396\n",
      "Epoch [   19/   30] | d_loss: 0.0001 | g_loss: 0.9995\n",
      "Epoch [   19/   30] | d_loss: 0.0003 | g_loss: 0.9982\n",
      "Epoch [   19/   30] | d_loss: 0.0001 | g_loss: 1.0068\n",
      "Epoch [   19/   30] | d_loss: 0.0004 | g_loss: 0.9781\n",
      "Epoch [   19/   30] | d_loss: 0.0001 | g_loss: 1.0133\n",
      "Epoch [   19/   30] | d_loss: 0.0002 | g_loss: 0.9925\n",
      "Epoch [   19/   30] | d_loss: 0.0000 | g_loss: 1.0017\n",
      "Epoch [   19/   30] | d_loss: 0.0001 | g_loss: 0.9971\n",
      "Epoch [   19/   30] | d_loss: 0.0001 | g_loss: 1.0056\n",
      "Epoch [   19/   30] | d_loss: 0.0001 | g_loss: 0.9976\n",
      "Epoch [   19/   30] | d_loss: 0.0002 | g_loss: 0.9777\n",
      "Epoch [   19/   30] | d_loss: 0.0001 | g_loss: 1.0107\n",
      "Epoch [   19/   30] | d_loss: 0.0004 | g_loss: 1.0092\n",
      "Epoch [   19/   30] | d_loss: 0.0008 | g_loss: 0.9807\n",
      "Epoch [   19/   30] | d_loss: 0.0001 | g_loss: 0.9957\n",
      "Epoch [   20/   30] | d_loss: 0.0001 | g_loss: 1.0035\n",
      "Epoch [   20/   30] | d_loss: 0.0002 | g_loss: 1.0083\n",
      "Epoch [   20/   30] | d_loss: 0.0001 | g_loss: 0.9958\n",
      "Epoch [   20/   30] | d_loss: 0.0002 | g_loss: 0.9975\n",
      "Epoch [   20/   30] | d_loss: 0.0003 | g_loss: 0.9829\n",
      "Epoch [   20/   30] | d_loss: 0.0001 | g_loss: 1.0051\n",
      "Epoch [   20/   30] | d_loss: 0.0008 | g_loss: 1.0249\n",
      "Epoch [   20/   30] | d_loss: 0.0001 | g_loss: 0.9936\n",
      "Epoch [   20/   30] | d_loss: 0.0002 | g_loss: 1.0105\n",
      "Epoch [   20/   30] | d_loss: 0.0002 | g_loss: 0.9902\n",
      "Epoch [   20/   30] | d_loss: 0.0019 | g_loss: 1.0325\n",
      "Epoch [   20/   30] | d_loss: 0.0001 | g_loss: 1.0053\n",
      "Epoch [   20/   30] | d_loss: 0.0001 | g_loss: 1.0026\n",
      "Epoch [   20/   30] | d_loss: 0.0002 | g_loss: 0.9886\n",
      "Epoch [   20/   30] | d_loss: 0.0001 | g_loss: 0.9906\n",
      "Epoch [   20/   30] | d_loss: 0.0006 | g_loss: 0.9826\n",
      "Epoch [   20/   30] | d_loss: 0.0007 | g_loss: 0.9828\n",
      "Epoch [   20/   30] | d_loss: 0.0004 | g_loss: 1.0118\n",
      "Epoch [   20/   30] | d_loss: 0.0003 | g_loss: 0.9904\n",
      "Epoch [   20/   30] | d_loss: 0.0001 | g_loss: 0.9949\n",
      "Epoch [   20/   30] | d_loss: 0.0011 | g_loss: 0.9734\n",
      "Epoch [   20/   30] | d_loss: 0.0003 | g_loss: 0.9794\n",
      "Epoch [   20/   30] | d_loss: 0.0016 | g_loss: 0.9545\n",
      "Epoch [   20/   30] | d_loss: 0.0007 | g_loss: 1.0132\n",
      "Epoch [   20/   30] | d_loss: 0.0009 | g_loss: 1.0154\n",
      "Epoch [   20/   30] | d_loss: 0.0003 | g_loss: 1.0044\n",
      "Epoch [   20/   30] | d_loss: 0.0001 | g_loss: 1.0003\n",
      "Epoch [   20/   30] | d_loss: 0.0007 | g_loss: 1.0090\n",
      "Epoch [   20/   30] | d_loss: 0.0012 | g_loss: 1.0133\n",
      "Epoch [   21/   30] | d_loss: 0.0004 | g_loss: 0.9925\n",
      "Epoch [   21/   30] | d_loss: 0.0002 | g_loss: 1.0017\n",
      "Epoch [   21/   30] | d_loss: 0.0010 | g_loss: 0.9893\n",
      "Epoch [   21/   30] | d_loss: 0.0001 | g_loss: 1.0033\n",
      "Epoch [   21/   30] | d_loss: 0.0001 | g_loss: 0.9955\n",
      "Epoch [   21/   30] | d_loss: 0.0001 | g_loss: 1.0013\n",
      "Epoch [   21/   30] | d_loss: 0.0002 | g_loss: 1.0061\n",
      "Epoch [   21/   30] | d_loss: 0.0002 | g_loss: 1.0058\n",
      "Epoch [   21/   30] | d_loss: 0.0001 | g_loss: 1.0020\n",
      "Epoch [   21/   30] | d_loss: 0.0002 | g_loss: 0.9961\n",
      "Epoch [   21/   30] | d_loss: 0.0001 | g_loss: 1.0034\n",
      "Epoch [   21/   30] | d_loss: 0.0001 | g_loss: 1.0019\n",
      "Epoch [   21/   30] | d_loss: 0.0002 | g_loss: 1.0070\n",
      "Epoch [   21/   30] | d_loss: 0.0000 | g_loss: 1.0012\n",
      "Epoch [   21/   30] | d_loss: 0.0001 | g_loss: 1.0028\n",
      "Epoch [   21/   30] | d_loss: 0.0000 | g_loss: 0.9997\n",
      "Epoch [   21/   30] | d_loss: 0.0000 | g_loss: 0.9996\n",
      "Epoch [   21/   30] | d_loss: 0.0001 | g_loss: 0.9961\n",
      "Epoch [   21/   30] | d_loss: 0.0000 | g_loss: 1.0023\n",
      "Epoch [   21/   30] | d_loss: 0.0000 | g_loss: 1.0001\n",
      "Epoch [   21/   30] | d_loss: 0.0002 | g_loss: 1.0071\n",
      "Epoch [   21/   30] | d_loss: 0.0001 | g_loss: 1.0031\n",
      "Epoch [   21/   30] | d_loss: 0.0001 | g_loss: 1.0037\n",
      "Epoch [   21/   30] | d_loss: 0.0003 | g_loss: 1.0078\n",
      "Epoch [   21/   30] | d_loss: 0.0001 | g_loss: 0.9922\n",
      "Epoch [   21/   30] | d_loss: 0.0000 | g_loss: 0.9957\n",
      "Epoch [   21/   30] | d_loss: 0.0000 | g_loss: 1.0032\n",
      "Epoch [   21/   30] | d_loss: 0.0001 | g_loss: 0.9933\n",
      "Epoch [   21/   30] | d_loss: 0.0000 | g_loss: 0.9980\n",
      "Epoch [   22/   30] | d_loss: 0.0007 | g_loss: 0.9881\n",
      "Epoch [   22/   30] | d_loss: 0.0000 | g_loss: 0.9980\n",
      "Epoch [   22/   30] | d_loss: 0.0000 | g_loss: 0.9985\n",
      "Epoch [   22/   30] | d_loss: 0.0001 | g_loss: 0.9957\n",
      "Epoch [   22/   30] | d_loss: 0.0003 | g_loss: 1.0124\n",
      "Epoch [   22/   30] | d_loss: 0.0002 | g_loss: 0.9998\n",
      "Epoch [   22/   30] | d_loss: 0.0001 | g_loss: 0.9977\n",
      "Epoch [   22/   30] | d_loss: 0.0001 | g_loss: 0.9903\n",
      "Epoch [   22/   30] | d_loss: 0.0001 | g_loss: 0.9862\n",
      "Epoch [   22/   30] | d_loss: 0.0000 | g_loss: 0.9989\n",
      "Epoch [   22/   30] | d_loss: 0.0009 | g_loss: 0.9955\n",
      "Epoch [   22/   30] | d_loss: 0.0015 | g_loss: 0.9881\n",
      "Epoch [   22/   30] | d_loss: 0.0004 | g_loss: 0.9984\n",
      "Epoch [   22/   30] | d_loss: 0.0013 | g_loss: 0.9974\n",
      "Epoch [   22/   30] | d_loss: 0.0002 | g_loss: 1.0057\n",
      "Epoch [   22/   30] | d_loss: 0.0001 | g_loss: 0.9974\n",
      "Epoch [   22/   30] | d_loss: 0.0005 | g_loss: 1.0113\n",
      "Epoch [   22/   30] | d_loss: 0.0005 | g_loss: 1.0260\n",
      "Epoch [   22/   30] | d_loss: 0.0014 | g_loss: 0.9569\n",
      "Epoch [   22/   30] | d_loss: 0.0004 | g_loss: 0.9824\n",
      "Epoch [   22/   30] | d_loss: 0.0002 | g_loss: 0.9944\n",
      "Epoch [   22/   30] | d_loss: 0.0002 | g_loss: 1.0102\n",
      "Epoch [   22/   30] | d_loss: 0.0002 | g_loss: 0.9856\n",
      "Epoch [   22/   30] | d_loss: 0.0002 | g_loss: 1.0117\n",
      "Epoch [   22/   30] | d_loss: 0.0001 | g_loss: 1.0102\n",
      "Epoch [   22/   30] | d_loss: 0.0012 | g_loss: 0.9678\n",
      "Epoch [   22/   30] | d_loss: 0.0002 | g_loss: 0.9943\n",
      "Epoch [   22/   30] | d_loss: 0.0003 | g_loss: 1.0529\n",
      "Epoch [   22/   30] | d_loss: 0.0001 | g_loss: 1.0021\n",
      "Epoch [   23/   30] | d_loss: 0.0004 | g_loss: 0.9892\n",
      "Epoch [   23/   30] | d_loss: 0.0001 | g_loss: 1.0193\n",
      "Epoch [   23/   30] | d_loss: 0.0001 | g_loss: 0.9907\n",
      "Epoch [   23/   30] | d_loss: 0.0002 | g_loss: 1.0105\n",
      "Epoch [   23/   30] | d_loss: 0.0002 | g_loss: 0.9976\n",
      "Epoch [   23/   30] | d_loss: 0.0001 | g_loss: 1.0154\n",
      "Epoch [   23/   30] | d_loss: 0.0002 | g_loss: 1.0007\n",
      "Epoch [   23/   30] | d_loss: 0.0003 | g_loss: 0.9788\n",
      "Epoch [   23/   30] | d_loss: 0.0001 | g_loss: 1.0058\n",
      "Epoch [   23/   30] | d_loss: 0.0015 | g_loss: 0.9464\n",
      "Epoch [   23/   30] | d_loss: 0.0002 | g_loss: 1.0226\n",
      "Epoch [   23/   30] | d_loss: 0.0001 | g_loss: 1.0104\n",
      "Epoch [   23/   30] | d_loss: 0.0002 | g_loss: 1.0055\n",
      "Epoch [   23/   30] | d_loss: 0.0000 | g_loss: 0.9993\n",
      "Epoch [   23/   30] | d_loss: 0.0001 | g_loss: 1.0052\n",
      "Epoch [   23/   30] | d_loss: 0.0001 | g_loss: 1.0050\n",
      "Epoch [   23/   30] | d_loss: 0.0003 | g_loss: 0.9993\n",
      "Epoch [   23/   30] | d_loss: 0.0001 | g_loss: 1.0021\n",
      "Epoch [   23/   30] | d_loss: 0.0002 | g_loss: 1.0192\n",
      "Epoch [   23/   30] | d_loss: 0.0001 | g_loss: 0.9969\n",
      "Epoch [   23/   30] | d_loss: 0.0002 | g_loss: 0.9924\n",
      "Epoch [   23/   30] | d_loss: 0.0001 | g_loss: 1.0032\n",
      "Epoch [   23/   30] | d_loss: 0.0001 | g_loss: 0.9954\n",
      "Epoch [   23/   30] | d_loss: 0.0001 | g_loss: 0.9993\n",
      "Epoch [   23/   30] | d_loss: 0.0002 | g_loss: 0.9932\n",
      "Epoch [   23/   30] | d_loss: 0.0001 | g_loss: 1.0040\n",
      "Epoch [   23/   30] | d_loss: 0.0001 | g_loss: 0.9994\n",
      "Epoch [   23/   30] | d_loss: 0.0005 | g_loss: 0.9822\n",
      "Epoch [   23/   30] | d_loss: 0.0001 | g_loss: 1.0065\n",
      "Epoch [   24/   30] | d_loss: 0.0087 | g_loss: 1.0688\n",
      "Epoch [   24/   30] | d_loss: 0.0002 | g_loss: 1.0085\n",
      "Epoch [   24/   30] | d_loss: 0.0001 | g_loss: 1.0080\n",
      "Epoch [   24/   30] | d_loss: 0.0001 | g_loss: 0.9885\n",
      "Epoch [   24/   30] | d_loss: 0.0001 | g_loss: 1.0016\n",
      "Epoch [   24/   30] | d_loss: 0.0001 | g_loss: 1.0023\n",
      "Epoch [   24/   30] | d_loss: 0.0001 | g_loss: 0.9945\n",
      "Epoch [   24/   30] | d_loss: 0.0000 | g_loss: 0.9977\n",
      "Epoch [   24/   30] | d_loss: 0.0001 | g_loss: 1.0115\n",
      "Epoch [   24/   30] | d_loss: 0.0000 | g_loss: 0.9992\n",
      "Epoch [   24/   30] | d_loss: 0.0002 | g_loss: 1.0138\n",
      "Epoch [   24/   30] | d_loss: 0.0001 | g_loss: 1.0074\n",
      "Epoch [   24/   30] | d_loss: 0.0001 | g_loss: 1.0157\n",
      "Epoch [   24/   30] | d_loss: 0.0001 | g_loss: 0.9961\n",
      "Epoch [   24/   30] | d_loss: 0.0002 | g_loss: 1.0092\n",
      "Epoch [   24/   30] | d_loss: 0.0002 | g_loss: 1.0102\n",
      "Epoch [   24/   30] | d_loss: 0.0000 | g_loss: 1.0037\n",
      "Epoch [   24/   30] | d_loss: 0.0001 | g_loss: 0.9972\n",
      "Epoch [   24/   30] | d_loss: 0.0000 | g_loss: 1.0009\n",
      "Epoch [   24/   30] | d_loss: 0.0004 | g_loss: 1.0088\n",
      "Epoch [   24/   30] | d_loss: 0.0001 | g_loss: 0.9938\n",
      "Epoch [   24/   30] | d_loss: 0.0001 | g_loss: 0.9903\n",
      "Epoch [   24/   30] | d_loss: 0.0001 | g_loss: 1.0038\n",
      "Epoch [   24/   30] | d_loss: 0.0000 | g_loss: 1.0013\n",
      "Epoch [   24/   30] | d_loss: 0.0010 | g_loss: 1.0279\n",
      "Epoch [   24/   30] | d_loss: 0.0004 | g_loss: 0.9853\n",
      "Epoch [   24/   30] | d_loss: 0.0001 | g_loss: 0.9952\n",
      "Epoch [   24/   30] | d_loss: 0.0001 | g_loss: 1.0041\n",
      "Epoch [   24/   30] | d_loss: 0.0001 | g_loss: 0.9931\n",
      "Epoch [   25/   30] | d_loss: 0.0008 | g_loss: 1.0235\n",
      "Epoch [   25/   30] | d_loss: 0.0001 | g_loss: 0.9916\n",
      "Epoch [   25/   30] | d_loss: 0.0002 | g_loss: 0.9884\n",
      "Epoch [   25/   30] | d_loss: 0.0001 | g_loss: 0.9864\n",
      "Epoch [   25/   30] | d_loss: 0.0001 | g_loss: 1.0024\n",
      "Epoch [   25/   30] | d_loss: 0.0002 | g_loss: 0.9881\n",
      "Epoch [   25/   30] | d_loss: 0.0000 | g_loss: 1.0011\n",
      "Epoch [   25/   30] | d_loss: 0.0001 | g_loss: 0.9979\n",
      "Epoch [   25/   30] | d_loss: 0.0001 | g_loss: 0.9947\n",
      "Epoch [   25/   30] | d_loss: 0.0001 | g_loss: 0.9971\n",
      "Epoch [   25/   30] | d_loss: 0.0010 | g_loss: 1.0200\n",
      "Epoch [   25/   30] | d_loss: 0.0001 | g_loss: 1.0022\n",
      "Epoch [   25/   30] | d_loss: 0.0000 | g_loss: 0.9994\n",
      "Epoch [   25/   30] | d_loss: 0.0001 | g_loss: 0.9944\n",
      "Epoch [   25/   30] | d_loss: 0.0005 | g_loss: 0.9903\n",
      "Epoch [   25/   30] | d_loss: 0.0001 | g_loss: 1.0019\n",
      "Epoch [   25/   30] | d_loss: 0.0001 | g_loss: 0.9970\n",
      "Epoch [   25/   30] | d_loss: 0.0001 | g_loss: 0.9991\n",
      "Epoch [   25/   30] | d_loss: 0.0001 | g_loss: 1.0012\n",
      "Epoch [   25/   30] | d_loss: 0.0001 | g_loss: 0.9979\n",
      "Epoch [   25/   30] | d_loss: 0.0003 | g_loss: 0.9930\n",
      "Epoch [   25/   30] | d_loss: 0.0005 | g_loss: 0.9937\n",
      "Epoch [   25/   30] | d_loss: 0.0000 | g_loss: 0.9968\n",
      "Epoch [   25/   30] | d_loss: 0.0002 | g_loss: 0.9943\n",
      "Epoch [   25/   30] | d_loss: 0.0002 | g_loss: 0.9991\n",
      "Epoch [   25/   30] | d_loss: 0.0002 | g_loss: 0.9955\n",
      "Epoch [   25/   30] | d_loss: 0.0002 | g_loss: 1.0051\n",
      "Epoch [   25/   30] | d_loss: 0.0004 | g_loss: 1.0076\n",
      "Epoch [   25/   30] | d_loss: 0.0000 | g_loss: 0.9976\n",
      "Epoch [   26/   30] | d_loss: 0.0002 | g_loss: 0.9885\n",
      "Epoch [   26/   30] | d_loss: 0.0003 | g_loss: 0.9872\n",
      "Epoch [   26/   30] | d_loss: 0.0000 | g_loss: 0.9951\n",
      "Epoch [   26/   30] | d_loss: 0.0002 | g_loss: 0.9935\n",
      "Epoch [   26/   30] | d_loss: 0.0001 | g_loss: 1.0019\n",
      "Epoch [   26/   30] | d_loss: 0.0002 | g_loss: 0.9940\n",
      "Epoch [   26/   30] | d_loss: 0.0002 | g_loss: 0.9922\n",
      "Epoch [   26/   30] | d_loss: 0.0000 | g_loss: 0.9976\n",
      "Epoch [   26/   30] | d_loss: 0.0068 | g_loss: 0.9940\n",
      "Epoch [   26/   30] | d_loss: 0.0033 | g_loss: 0.9281\n",
      "Epoch [   26/   30] | d_loss: 0.0021 | g_loss: 1.0400\n",
      "Epoch [   26/   30] | d_loss: 0.0018 | g_loss: 0.9812\n",
      "Epoch [   26/   30] | d_loss: 0.0014 | g_loss: 0.9438\n",
      "Epoch [   26/   30] | d_loss: 0.0006 | g_loss: 1.0045\n",
      "Epoch [   26/   30] | d_loss: 0.0004 | g_loss: 0.9995\n",
      "Epoch [   26/   30] | d_loss: 0.0032 | g_loss: 1.1824\n",
      "Epoch [   26/   30] | d_loss: 0.0008 | g_loss: 0.9847\n",
      "Epoch [   26/   30] | d_loss: 0.0005 | g_loss: 1.0267\n",
      "Epoch [   26/   30] | d_loss: 0.0008 | g_loss: 1.0332\n",
      "Epoch [   26/   30] | d_loss: 0.0632 | g_loss: 1.8475\n",
      "Epoch [   26/   30] | d_loss: 0.0009 | g_loss: 0.9784\n",
      "Epoch [   26/   30] | d_loss: 0.0008 | g_loss: 1.0267\n",
      "Epoch [   26/   30] | d_loss: 0.0009 | g_loss: 0.9992\n",
      "Epoch [   26/   30] | d_loss: 0.0014 | g_loss: 1.0381\n",
      "Epoch [   26/   30] | d_loss: 0.0054 | g_loss: 1.1152\n",
      "Epoch [   26/   30] | d_loss: 0.0035 | g_loss: 1.1127\n",
      "Epoch [   26/   30] | d_loss: 0.0037 | g_loss: 0.8931\n",
      "Epoch [   26/   30] | d_loss: 0.0005 | g_loss: 1.0033\n",
      "Epoch [   26/   30] | d_loss: 0.0014 | g_loss: 0.9877\n",
      "Epoch [   27/   30] | d_loss: 0.0012 | g_loss: 1.0538\n",
      "Epoch [   27/   30] | d_loss: 0.0005 | g_loss: 1.0255\n",
      "Epoch [   27/   30] | d_loss: 0.0006 | g_loss: 0.9698\n",
      "Epoch [   27/   30] | d_loss: 0.0007 | g_loss: 1.0292\n",
      "Epoch [   27/   30] | d_loss: 0.0017 | g_loss: 0.9622\n",
      "Epoch [   27/   30] | d_loss: 0.0008 | g_loss: 0.9933\n",
      "Epoch [   27/   30] | d_loss: 0.0003 | g_loss: 1.0015\n",
      "Epoch [   27/   30] | d_loss: 0.0004 | g_loss: 1.0003\n",
      "Epoch [   27/   30] | d_loss: 0.0003 | g_loss: 1.0128\n",
      "Epoch [   27/   30] | d_loss: 0.0009 | g_loss: 1.0426\n",
      "Epoch [   27/   30] | d_loss: 0.0036 | g_loss: 0.9129\n",
      "Epoch [   27/   30] | d_loss: 0.0002 | g_loss: 0.9979\n",
      "Epoch [   27/   30] | d_loss: 0.0016 | g_loss: 1.0684\n",
      "Epoch [   27/   30] | d_loss: 0.0005 | g_loss: 0.9891\n",
      "Epoch [   27/   30] | d_loss: 0.0002 | g_loss: 0.9853\n",
      "Epoch [   27/   30] | d_loss: 0.0005 | g_loss: 0.9802\n",
      "Epoch [   27/   30] | d_loss: 0.0007 | g_loss: 1.0300\n",
      "Epoch [   27/   30] | d_loss: 0.0005 | g_loss: 1.0203\n",
      "Epoch [   27/   30] | d_loss: 0.0005 | g_loss: 0.9747\n",
      "Epoch [   27/   30] | d_loss: 0.0132 | g_loss: 1.1269\n",
      "Epoch [   27/   30] | d_loss: 0.0008 | g_loss: 1.0425\n",
      "Epoch [   27/   30] | d_loss: 0.0015 | g_loss: 0.9332\n",
      "Epoch [   27/   30] | d_loss: 0.0007 | g_loss: 1.0799\n",
      "Epoch [   27/   30] | d_loss: 0.0013 | g_loss: 1.0584\n",
      "Epoch [   27/   30] | d_loss: 0.0036 | g_loss: 1.0772\n",
      "Epoch [   27/   30] | d_loss: 0.0003 | g_loss: 1.0104\n",
      "Epoch [   27/   30] | d_loss: 0.0005 | g_loss: 1.0254\n",
      "Epoch [   27/   30] | d_loss: 0.0006 | g_loss: 0.9665\n",
      "Epoch [   27/   30] | d_loss: 0.0003 | g_loss: 0.9894\n",
      "Epoch [   28/   30] | d_loss: 0.0001 | g_loss: 0.9986\n",
      "Epoch [   28/   30] | d_loss: 0.0002 | g_loss: 1.0214\n",
      "Epoch [   28/   30] | d_loss: 0.0008 | g_loss: 0.9645\n",
      "Epoch [   28/   30] | d_loss: 0.0022 | g_loss: 0.9027\n",
      "Epoch [   28/   30] | d_loss: 0.0004 | g_loss: 0.9760\n",
      "Epoch [   28/   30] | d_loss: 0.0004 | g_loss: 0.9745\n",
      "Epoch [   28/   30] | d_loss: 0.0007 | g_loss: 0.9743\n",
      "Epoch [   28/   30] | d_loss: 0.0019 | g_loss: 0.9379\n",
      "Epoch [   28/   30] | d_loss: 0.0001 | g_loss: 1.0100\n",
      "Epoch [   28/   30] | d_loss: 0.0001 | g_loss: 1.0057\n",
      "Epoch [   28/   30] | d_loss: 0.0009 | g_loss: 0.9592\n",
      "Epoch [   28/   30] | d_loss: 0.0001 | g_loss: 0.9858\n",
      "Epoch [   28/   30] | d_loss: 0.0001 | g_loss: 1.0039\n",
      "Epoch [   28/   30] | d_loss: 0.0005 | g_loss: 0.9738\n",
      "Epoch [   28/   30] | d_loss: 0.0004 | g_loss: 0.9911\n",
      "Epoch [   28/   30] | d_loss: 0.0005 | g_loss: 1.0283\n",
      "Epoch [   28/   30] | d_loss: 0.0007 | g_loss: 1.0475\n",
      "Epoch [   28/   30] | d_loss: 0.0005 | g_loss: 0.9795\n",
      "Epoch [   28/   30] | d_loss: 0.0004 | g_loss: 0.9523\n",
      "Epoch [   28/   30] | d_loss: 0.0001 | g_loss: 0.9963\n",
      "Epoch [   28/   30] | d_loss: 0.0001 | g_loss: 0.9991\n",
      "Epoch [   28/   30] | d_loss: 0.0006 | g_loss: 0.9664\n",
      "Epoch [   28/   30] | d_loss: 0.0011 | g_loss: 0.9521\n",
      "Epoch [   28/   30] | d_loss: 0.0008 | g_loss: 0.9620\n",
      "Epoch [   28/   30] | d_loss: 0.0002 | g_loss: 0.9853\n",
      "Epoch [   28/   30] | d_loss: 0.0003 | g_loss: 1.0165\n",
      "Epoch [   28/   30] | d_loss: 0.0003 | g_loss: 0.9819\n",
      "Epoch [   28/   30] | d_loss: 0.0009 | g_loss: 1.0463\n",
      "Epoch [   28/   30] | d_loss: 0.0069 | g_loss: 1.3592\n",
      "Epoch [   29/   30] | d_loss: 0.0022 | g_loss: 1.0313\n",
      "Epoch [   29/   30] | d_loss: 0.0001 | g_loss: 1.0097\n",
      "Epoch [   29/   30] | d_loss: 0.0004 | g_loss: 1.0243\n",
      "Epoch [   29/   30] | d_loss: 0.0004 | g_loss: 0.9722\n",
      "Epoch [   29/   30] | d_loss: 0.0010 | g_loss: 0.9606\n",
      "Epoch [   29/   30] | d_loss: 0.0001 | g_loss: 0.9915\n",
      "Epoch [   29/   30] | d_loss: 0.0005 | g_loss: 1.0377\n",
      "Epoch [   29/   30] | d_loss: 0.0001 | g_loss: 0.9997\n",
      "Epoch [   29/   30] | d_loss: 0.0002 | g_loss: 0.9851\n",
      "Epoch [   29/   30] | d_loss: 0.0027 | g_loss: 0.9211\n",
      "Epoch [   29/   30] | d_loss: 0.0001 | g_loss: 0.9991\n",
      "Epoch [   29/   30] | d_loss: 0.0004 | g_loss: 0.9706\n",
      "Epoch [   29/   30] | d_loss: 0.0001 | g_loss: 0.9905\n",
      "Epoch [   29/   30] | d_loss: 0.0004 | g_loss: 1.0325\n",
      "Epoch [   29/   30] | d_loss: 0.0002 | g_loss: 1.0016\n",
      "Epoch [   29/   30] | d_loss: 0.0003 | g_loss: 0.9724\n",
      "Epoch [   29/   30] | d_loss: 0.0001 | g_loss: 0.9938\n",
      "Epoch [   29/   30] | d_loss: 0.0001 | g_loss: 0.9891\n",
      "Epoch [   29/   30] | d_loss: 0.0006 | g_loss: 0.9809\n",
      "Epoch [   29/   30] | d_loss: 0.0001 | g_loss: 0.9864\n",
      "Epoch [   29/   30] | d_loss: 0.0003 | g_loss: 0.9825\n",
      "Epoch [   29/   30] | d_loss: 0.0003 | g_loss: 0.9759\n",
      "Epoch [   29/   30] | d_loss: 0.0006 | g_loss: 0.9882\n",
      "Epoch [   29/   30] | d_loss: 0.0003 | g_loss: 1.0298\n",
      "Epoch [   29/   30] | d_loss: 0.0011 | g_loss: 1.0488\n",
      "Epoch [   29/   30] | d_loss: 0.0002 | g_loss: 1.0125\n",
      "Epoch [   29/   30] | d_loss: 0.0047 | g_loss: 0.8794\n",
      "Epoch [   29/   30] | d_loss: 0.0000 | g_loss: 0.9995\n",
      "Epoch [   29/   30] | d_loss: 0.0009 | g_loss: 0.9765\n",
      "Epoch [   30/   30] | d_loss: 0.0003 | g_loss: 0.9713\n",
      "Epoch [   30/   30] | d_loss: 0.0004 | g_loss: 0.9831\n",
      "Epoch [   30/   30] | d_loss: 0.0002 | g_loss: 0.9943\n",
      "Epoch [   30/   30] | d_loss: 0.0003 | g_loss: 1.0219\n",
      "Epoch [   30/   30] | d_loss: 0.0003 | g_loss: 1.0310\n",
      "Epoch [   30/   30] | d_loss: 0.0008 | g_loss: 0.9552\n",
      "Epoch [   30/   30] | d_loss: 0.0001 | g_loss: 0.9943\n",
      "Epoch [   30/   30] | d_loss: 0.0001 | g_loss: 1.0150\n",
      "Epoch [   30/   30] | d_loss: 0.0054 | g_loss: 0.9864\n",
      "Epoch [   30/   30] | d_loss: 0.0059 | g_loss: 1.2588\n",
      "Epoch [   30/   30] | d_loss: 0.0009 | g_loss: 0.9827\n",
      "Epoch [   30/   30] | d_loss: 0.0003 | g_loss: 0.9923\n",
      "Epoch [   30/   30] | d_loss: 0.0004 | g_loss: 1.0176\n",
      "Epoch [   30/   30] | d_loss: 0.0001 | g_loss: 0.9879\n",
      "Epoch [   30/   30] | d_loss: 0.0004 | g_loss: 0.9721\n",
      "Epoch [   30/   30] | d_loss: 0.0001 | g_loss: 0.9919\n",
      "Epoch [   30/   30] | d_loss: 0.0008 | g_loss: 1.0733\n",
      "Epoch [   30/   30] | d_loss: 0.0001 | g_loss: 1.0057\n",
      "Epoch [   30/   30] | d_loss: 0.0193 | g_loss: 1.2980\n",
      "Epoch [   30/   30] | d_loss: 0.0002 | g_loss: 1.0077\n",
      "Epoch [   30/   30] | d_loss: 0.0002 | g_loss: 0.9891\n",
      "Epoch [   30/   30] | d_loss: 0.0001 | g_loss: 1.0008\n",
      "Epoch [   30/   30] | d_loss: 0.0001 | g_loss: 0.9984\n",
      "Epoch [   30/   30] | d_loss: 0.0002 | g_loss: 1.0076\n",
      "Epoch [   30/   30] | d_loss: 0.0000 | g_loss: 0.9946\n",
      "Epoch [   30/   30] | d_loss: 0.0002 | g_loss: 0.9877\n",
      "Epoch [   30/   30] | d_loss: 0.0003 | g_loss: 1.0108\n",
      "Epoch [   30/   30] | d_loss: 0.0002 | g_loss: 1.0184\n",
      "Epoch [   30/   30] | d_loss: 0.0002 | g_loss: 0.9832\n"
     ]
    }
   ],
   "source": [
    "# set number of epochs \n",
    "n_epochs = 30\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# call training function\n",
    "losses = train(D, G, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training loss\n",
    "\n",
    "Plot the training losses for the generator and discriminator, recorded after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7ff058289d68>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0561a8b38>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Discriminator', alpha=0.5)\n",
    "plt.plot(losses.T[1], label='Generator', alpha=0.5)\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generator samples from training\n",
    "\n",
    "View samples of images from the generator, and answer a question about the strengths and weaknesses of your trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for viewing a list of passed in sample images\n",
    "def view_samples(epoch, samples):\n",
    "    fig, axes = plt.subplots(figsize=(16,4), nrows=2, ncols=8, sharey=True, sharex=True)\n",
    "    for ax, img in zip(axes.flatten(), samples[epoch]):\n",
    "        img = img.detach().cpu().numpy()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        img = ((img + 1)*255 / (2)).astype(np.uint8)\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        im = ax.imshow(img.reshape((32,32,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load samples from generator, taken while training\n",
    "with open('train_samples.pkl', 'rb') as f:\n",
    "    samples = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0581fae48>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = view_samples(-1, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What do you notice about your generated samples and how might you improve this model?\n",
    "When you answer this question, consider the following factors:\n",
    "* The dataset is biased; it is made of \"celebrity\" faces that are mostly white\n",
    "* Model size; larger models have the opportunity to learn more features in a data feature space\n",
    "* Optimization strategy; optimizers and number of epochs affect your final result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** (Write your answer in this cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_face_generation.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\". Include the \"problem_unittests.py\" files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
